{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d3d2c76-7c52-496b-8c72-0850145e90b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import enum\n",
    "from functools import partial\n",
    "from typing import Callable, Optional, Sequence\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ActivationType(enum.Enum):\n",
    "    RELU = nn.ReLU\n",
    "    SELU = nn.SiLU\n",
    "\n",
    "# Script to see the deployment\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class MLPConfig:\n",
    "    input_dim: int\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "    hidden_depth: int\n",
    "    dropout_prob: Optional[float] = None\n",
    "    activation_fn: ActivationType = ActivationType.RELU\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"A feedforward multi-layer perceptron.\"\"\"\n",
    "\n",
    "    def __init__(self, config: MLPConfig) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        dropout_layer: Callable\n",
    "        if config.dropout_prob is not None:\n",
    "            dropout_layer = partial(nn.Dropout, p=config.dropout_prob)\n",
    "        else:\n",
    "            dropout_layer = nn.Identity\n",
    "\n",
    "        layers: Sequence[nn.Module]\n",
    "        if config.hidden_depth == 0:\n",
    "            layers = [nn.Linear(config.input_dim, config.output_dim)]\n",
    "        else:\n",
    "            layers = [\n",
    "                nn.Linear(config.input_dim, config.hidden_dim),\n",
    "                config.activation_fn.value(),\n",
    "                dropout_layer(),\n",
    "            ]\n",
    "            for _ in range(config.hidden_depth - 1):\n",
    "                layers += [\n",
    "                    nn.Linear(config.hidden_dim, config.hidden_dim),\n",
    "                    config.activation_fn.value(),\n",
    "                    dropout_layer(),\n",
    "                ]\n",
    "            layers += [nn.Linear(config.hidden_dim, config.output_dim)]\n",
    "        layers = [layer for layer in layers if not isinstance(layer, nn.Identity)]\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "    \n",
    "class EBMMLP(nn.Module):\n",
    "    def __init__(self, config: MLPConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        fused = torch.cat([x.unsqueeze(1).expand(-1, y.size(1), -1), y], dim=-1)\n",
    "        B, N, D = fused.size()\n",
    "        fused = fused.reshape(B * N, D).float()\n",
    "        out = self.mlp(fused)\n",
    "        return out.view(B,N)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f0d8274-3832-4c2c-b8d5-95a31d4c3f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(device, model_path, bc_model_type=None):\n",
    "    # Initialize the model\n",
    "    # TODO: Make all these initialization more general - init_learner and load_model!\n",
    "    input_dim = 1555  # We have a 1x1 conv that reduces to 16 channels.\n",
    "    action_dim = 23\n",
    "    mlp_config = MLPConfig(\n",
    "        input_dim=input_dim+action_dim,\n",
    "        hidden_dim=128,\n",
    "        output_dim=1,\n",
    "        hidden_depth=2,\n",
    "        dropout_prob=0,\n",
    "    )\n",
    "    model = EBMMLP(mlp_config)\n",
    "    state_dict = torch.load(model_path) # All the parameters by default gets installed to cuda 0\n",
    "    \n",
    "    model.load_state_dict(state_dict['model'])\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "430d6858-7e54-49b4-b2b6-ec0c5af45cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod_name: collections, name: OrderedDict\n",
      "mod_name: torch._utils, name: _rebuild_tensor_v2\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model(\n",
    "    device = torch.device('cuda'),\n",
    "    model_path = '/home/irmak/Workspace/ibc/experiments/tactile_implicit_ebm_10/checkpoints/ckpt_34000.ckpt',\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "296cc599-0d75-49a7-9ac8-69a7959c9e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EBMMLP(\n",
       "  (mlp): MLP(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=1578, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0, inplace=False)\n",
       "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "344205d0-0852-44b0-a10e-823ffd6427be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DerivativeFreeOptimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m     train_samples: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m     24\u001b[0m     inference_samples: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;129m@dataclasses\u001b[39m\u001b[38;5;241m.\u001b[39mdataclass\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDerivativeFreeOptimizer\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124;03m\"\"\"A simple derivative-free optimizer. Great for up to 5 dimensions.\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     device: torch\u001b[38;5;241m.\u001b[39mdevice\n",
      "Cell \u001b[0;32mIn [29], line 41\u001b[0m, in \u001b[0;36mDerivativeFreeOptimizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m inference_samples: \u001b[38;5;28mint\u001b[39m \n\u001b[1;32m     36\u001b[0m bounds: np\u001b[38;5;241m.\u001b[39mndarray\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize\u001b[39m(\n\u001b[1;32m     40\u001b[0m     config: DerivativeFreeConfig, device_type: \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m---> 41\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mDerivativeFreeOptimizer\u001b[49m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DerivativeFreeOptimizer(\n\u001b[1;32m     43\u001b[0m         device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(device_type \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     44\u001b[0m         noise_scale\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnoise_scale,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m         bounds\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbounds,\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sample\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_samples: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DerivativeFreeOptimizer' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class StochasticOptimizerConfig:\n",
    "    bounds: np.ndarray\n",
    "    \"\"\"Bounds on the samples, min/max for each dimension.\"\"\"\n",
    "\n",
    "    iters: int\n",
    "    \"\"\"The total number of inference iters.\"\"\"\n",
    "\n",
    "    train_samples: int\n",
    "    \"\"\"The number of counter-examples to sample per iter during training.\"\"\"\n",
    "\n",
    "    inference_samples: int\n",
    "    \"\"\"The number of candidates to sample per iter during inference.\"\"\"\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class DerivativeFreeConfig(StochasticOptimizerConfig):\n",
    "    noise_scale: float = 0.33\n",
    "    noise_shrink: float = 0.5\n",
    "    iters: int = 3\n",
    "    train_samples: int = 32\n",
    "    inference_samples: int = 64\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class DerivativeFreeOptimizer:\n",
    "    \"\"\"A simple derivative-free optimizer. Great for up to 5 dimensions.\"\"\"\n",
    "\n",
    "    device: torch.device\n",
    "    noise_scale: float\n",
    "    noise_shrink: float\n",
    "    iters: int\n",
    "    train_samples: int\n",
    "    inference_samples: int \n",
    "    bounds: np.ndarray\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize(\n",
    "        config: DerivativeFreeConfig, device_type: str\n",
    "    ) -> DerivativeFreeOptimizer:\n",
    "        return DerivativeFreeOptimizer(\n",
    "            device=torch.device(device_type if torch.cuda.is_available() else \"cpu\"),\n",
    "            noise_scale=config.noise_scale,\n",
    "            noise_shrink=config.noise_shrink,\n",
    "            iters=config.iters,\n",
    "            train_samples=config.train_samples,\n",
    "            inference_samples=config.inference_samples,\n",
    "            bounds=config.bounds,\n",
    "        )\n",
    "\n",
    "    def _sample(self, num_samples: int) -> torch.Tensor:\n",
    "        \"\"\"Helper method for drawing samples from the uniform random distribution.\"\"\"\n",
    "        # print('self.bounds: {}'.format(self.bounds))\n",
    "        size = (num_samples, self.bounds.shape[1])\n",
    "        samples = np.random.uniform(self.bounds[0, :], self.bounds[1, :], size=size)\n",
    "        return torch.as_tensor(samples, dtype=torch.float64, device=self.device)\n",
    "\n",
    "    def sample(self, batch_size: int, ebm: nn.Module) -> torch.Tensor:\n",
    "        del ebm  # The derivative-free optimizer does not use the ebm for sampling.\n",
    "        samples = self._sample(batch_size * self.train_samples)\n",
    "        return samples.reshape(batch_size, self.train_samples, -1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def infer(self, x: torch.Tensor, ebm: nn.Module) -> torch.Tensor:\n",
    "        \"\"\"Optimize for the best action given a trained EBM.\"\"\"\n",
    "        noise_scale = self.noise_scale\n",
    "        bounds = torch.as_tensor(self.bounds).to(self.device)\n",
    "\n",
    "        samples = self._sample(x.size(0) * self.inference_samples)\n",
    "        samples = samples.reshape(x.size(0), self.inference_samples, -1)\n",
    "\n",
    "        for i in range(self.iters):\n",
    "            # Compute energies.\n",
    "            energies = ebm(x, samples)\n",
    "            probs = F.softmax(-1.0 * energies, dim=-1)\n",
    "\n",
    "            # Resample with replacement.\n",
    "            idxs = torch.multinomial(probs, self.inference_samples, replacement=True)\n",
    "            samples = samples[torch.arange(samples.size(0)).unsqueeze(-1), idxs]\n",
    "\n",
    "            # Add noise and clip to target bounds.\n",
    "            samples = samples + torch.randn_like(samples) * noise_scale\n",
    "            samples = samples.clamp(min=bounds[0, :], max=bounds[1, :])\n",
    "\n",
    "            noise_scale *= self.noise_shrink\n",
    "\n",
    "        # Return target with highest probability.\n",
    "        energies = ebm(x, samples)\n",
    "        probs = F.softmax(-1.0 * energies, dim=-1)\n",
    "        best_idxs = probs.argmax(dim=-1)\n",
    "        return samples[torch.arange(samples.size(0)), best_idxs, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02148ae2-c504-46e4-8ee0-e4c975f0a03f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
