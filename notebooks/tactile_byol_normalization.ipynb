{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda22a78-ad76-44ba-995f-db0484f0391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to have different ways to train the tactile image\n",
    "import glob\n",
    "import h5py\n",
    "import hydra\n",
    "import mmap\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import torch.utils.data as data \n",
    "\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm \n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from holobot.robot.allegro.allegro_kdl import AllegroKDL\n",
    "\n",
    "from tactile_learning.utils.constants import *\n",
    "from tactile_learning.models.custom import *\n",
    "from tactile_learning.datasets.tactile_vision import *\n",
    "from tactile_learning.deployment.load_models import * \n",
    "from tactile_learning.deployment.nn_buffer import NearestNeighborBuffer\n",
    "from tactile_learning.models.knn import KNearestNeighbors, ScaledKNearestNeighbors\n",
    "from tactile_learning.utils.visualization import *\n",
    "from tactile_learning.utils.tactile_image import *\n",
    "from tactile_learning.utils.data import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1069ec0-8f05-4a48-b447-82522e876ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tactile normalization training for all possible encoder types: alexnet pretrained, resnet pretrained, stacked cnn and single sensor cnns\n",
    "# on top of previous implementations what is needed is to normalize them properly\n",
    "# So this script will:\n",
    "# 1. Find the mean and std of the current dataset\n",
    "# 2. Find the min and max of the normalized dataset with the found dataset\n",
    "# 3. Map the tactile images to bw 0,1 with these min and max\n",
    "# 4. Then find the mean and std again\n",
    "# 5. And use these mean and stds for BYOL training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "101d57f0-8ffc-4303-acd7-8e40e3ad2ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAY_DATA_DIR = '/home/irmak/Workspace/Holo-Bot/extracted_data/tactile_play_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fcad58b-3010-4397-a6be-e4d701a8588a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_val: tensor([[[0.0033]],\n",
      "\n",
      "        [[0.0005]],\n",
      "\n",
      "        [[0.0027]]]), max_val: tensor([[[0.9998]],\n",
      "\n",
      "        [[0.9996]],\n",
      "\n",
      "        [[0.9999]]])\n",
      "x: (tensor(0.9999), tensor(0.0005)), y: (tensor(1.), tensor(1.), tensor(1.), tensor(0.), tensor(0.), tensor(0.))\n"
     ]
    }
   ],
   "source": [
    "# x = torch.FloatTensor([[1,4,532,3,5,6,2,4,7,3],\n",
    "#                        [3,4,6,7,345,1,2,8,10,2],\n",
    "#                        [34,5,7,90,3,2,1,67,0,34]])\n",
    "x = torch.rand((2,3,16,16))\n",
    "class XStats:\n",
    "    def __init__(self, x):\n",
    "        self.min_val = torch.Tensor([x[:,i,:,:].min() for i in range(3)]).unsqueeze(1).unsqueeze(1)\n",
    "        self.max_val = torch.Tensor([x[:,i,:,:].max() for i in range(3)]).unsqueeze(1).unsqueeze(1)\n",
    "        print('min_val: {}, max_val: {}'.format(self.min_val, self.max_val))\n",
    "        self.transform = T.Lambda(self._scale_transform)\n",
    "    def _scale_transform(self, x):\n",
    "        x = (x-self.min_val) / (self.max_val - self.min_val)\n",
    "        return x\n",
    "\n",
    "    def getitem(self, x):\n",
    "        return self.transform(x)\n",
    "    \n",
    "stats = XStats(x)\n",
    "y = stats.getitem(x)\n",
    "print(f'x: {x.max(), x.min()}, y: {y[:,0,:,:].max(),y[:,1,:,:].max(),y[:,2,:,:].max(), y[:,0,:,:].min(),y[:,1,:,:].min(),y[:,2,:,:].min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fc33028-04bf-4633-9bac-4b166f3dd611",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TactileBYOLDataset(data.Dataset):\n",
    "    # Dataset for all possible tactile types (stacked, whole hand, one sensor)\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        tactile_information_type, # It could be either one of - stacked, whole_hand, single_sensor\n",
    "        img_size,\n",
    "        mean_std=None, # This is a general stats for all tactile information\n",
    "        min_max=None # Minimum and maximum of the tactile dataset - if given none these values should be found by using this dataset\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.roots = glob.glob(f'{data_path}/demonstration_*')\n",
    "        self.roots = sorted(self.roots)\n",
    "        self.data = load_data(self.roots, demos_to_use=[])\n",
    "        assert tactile_information_type in ['stacked', 'whole_hand', 'single_sensor'], 'tactile_information_type can either be \"stacked\", \"whole_hand\" or \"single_sensor\"'\n",
    "        self.tactile_information_type = tactile_information_type\n",
    "        \n",
    "        # Set the transforms accordingly\n",
    "        self.img_size = img_size\n",
    "        self.min_max = min_max \n",
    "        self.mean_std = mean_std\n",
    "            \n",
    "        if mean_std is None:\n",
    "            self.transform = T.Resize(img_size)\n",
    "        elif min_max is None:\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize(img_size),\n",
    "                T.Normalize(mean_std[0], mean_std[1])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize(img_size),\n",
    "                T.Normalize(mean_std[0], mean_std[1]),\n",
    "                T.Lambda(self._scale_transform)\n",
    "            ])\n",
    "            \n",
    "        # Set the indices for one sensor\n",
    "        if tactile_information_type == 'single_sensor':\n",
    "            self._preprocess_tactile_indices()\n",
    "            \n",
    "        # Set up the tactile image retrieval function\n",
    "        if tactile_information_type == 'single_sensor':\n",
    "            self._get_tactile_image = self._get_single_sensor_tactile_image\n",
    "        elif tactile_information_type == 'stacked':\n",
    "            self._get_tactile_image = self._get_stacked_tactile_image\n",
    "        elif tactile_information_type == 'whole_hand':\n",
    "            self._get_tactile_image = self._get_whole_hand_tactile_image\n",
    "            \n",
    "    def _preprocess_tactile_indices(self):\n",
    "        self.tactile_mapper = np.zeros(len(self.data['tactile']['indices'])*15).astype(int)\n",
    "        for data_id in range(len(self.data['tactile']['indices'])):\n",
    "            for sensor_id in range(15):\n",
    "                self.tactile_mapper[data_id*15+sensor_id] = data_id # Assign each finger to an index basically\n",
    "\n",
    "    def _get_sensor_id(self, index):\n",
    "        return index % 15\n",
    "            \n",
    "    def _scale_transform(self, image): # Transform function to map the image between 0 and 1\n",
    "        image = (image - self.min_max[0]) / (self.min_max[1] - self.min_max[0])\n",
    "        return image\n",
    "            \n",
    "    def _get_whole_hand_tactile_image(self, tactile_values): \n",
    "        # tactile_values: (15,16,3) - turn it into 16,16,3 by concatenating 0z\n",
    "        tactile_image = torch.FloatTensor(tactile_values)\n",
    "        tactile_image = F.pad(tactile_image, (0,0,0,0,1,0), 'constant', 0)\n",
    "        # reshape it to 4x4\n",
    "        tactile_image = tactile_image.view(16,4,4,3)\n",
    "\n",
    "        # concat for it have its proper shape\n",
    "        tactile_image = torch.concat([\n",
    "            torch.concat([tactile_image[i*4+j] for j in range(4)], dim=0)\n",
    "            for i in range(4)\n",
    "        ], dim=1)\n",
    "\n",
    "        tactile_image = torch.permute(tactile_image, (2,0,1))\n",
    "        \n",
    "        return self.transform(tactile_image)\n",
    "    \n",
    "    def _get_stacked_tactile_image(self, tactile_values):\n",
    "        tactile_image = torch.FloatTensor(tactile_values)\n",
    "        tactile_image = tactile_image.view(15,4,4,3) # Just making sure that everything stays the same\n",
    "        tactile_image = torch.permute(tactile_image, (0,3,1,2))\n",
    "        tactile_image = tactile_image.reshape(-1,4,4) # Make 45 the channel number \n",
    "        return self.transform(tactile_image)\n",
    "    \n",
    "    def _get_single_sensor_tactile_image(self, tactile_value):\n",
    "        tactile_image = torch.FloatTensor(tactile_value) # tactile_value.shape: (16,3)\n",
    "        tactile_image = tactile_image.view(4,4,3)\n",
    "        tactile_image = torch.permute(tactile_image, (2,0,1))\n",
    "        return self.transform(tactile_image)\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.tactile_information_type == 'single_sensor':\n",
    "            return len(self.tactile_mapper)\n",
    "        else: \n",
    "            return len(self.data['tactile']['indices'])\n",
    "        \n",
    "    def _get_proper_tactile_value(self, index):\n",
    "        if self.tactile_information_type == 'single_sensor':\n",
    "            data_id = self.tactile_mapper[index]\n",
    "            demo_id, tactile_id = self.data['tactile']['indices'][data_id]\n",
    "            sensor_id = self._get_sensor_id(index)\n",
    "            tactile_value = self.data['tactile']['values'][demo_id][tactile_id][sensor_id]\n",
    "            \n",
    "            return tactile_value\n",
    "        \n",
    "        else:\n",
    "            demo_id, tactile_id = self.data['tactile']['indices'][index]\n",
    "            tactile_values = self.data['tactile']['values'][demo_id][tactile_id]\n",
    "            \n",
    "            return tactile_values\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tactile_value = self._get_proper_tactile_value(index)\n",
    "        tactile_image = self._get_tactile_image(tactile_value)\n",
    "        \n",
    "        return tactile_image\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d351905-ed18-40be-85cb-c5d10bf16adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_hand_dset = TactileBYOLDataset(\n",
    "    data_path = PLAY_DATA_DIR,\n",
    "    tactile_information_type = 'whole_hand', # It could be either one of - stacked, whole_hand, single_sensor\n",
    "    img_size = 16,\n",
    "    mean_std=None, # This is a general stats for all tactile information\n",
    "    min_max=None # Minimum and maximum of the tactile dataset - if given none these values should be found by using this dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35357f2c-cf04-4a33-b6b2-d794505085f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_whole_hand_dset = len(whole_hand_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "831bd5d9-16e9-4120-bbcf-67cd797d2ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_stats(len_image_dataset, image_loader, img_size):\n",
    "    psum    = torch.tensor([0.0, 0.0, 0.0])\n",
    "    psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "\n",
    "    # loop through images\n",
    "    for inputs in tqdm(image_loader):\n",
    "        psum    += inputs.sum(axis = [0, 2, 3])\n",
    "        psum_sq += (inputs ** 2).sum(axis = [0, 2, 3])\n",
    "\n",
    "    # pixel count\n",
    "    count = len_image_dataset * img_size * img_size\n",
    "\n",
    "    # mean and std\n",
    "    total_mean = psum / count\n",
    "    total_var  = (psum_sq / count) - (total_mean ** 2)\n",
    "    total_std  = torch.sqrt(total_var)\n",
    "\n",
    "    # output\n",
    "    print('mean: '  + str(total_mean))\n",
    "    print('std:  '  + str(total_std))\n",
    "    \n",
    "    return total_mean, total_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e56a3a0-5519-4777-9c3f-4c4d08b20a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████| 3481/3481 [00:06<00:00, 524.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor([ 0.1218,  0.7076, 12.5459])\n",
      "std:  tensor([168.1889,  98.6576, 104.8352])\n"
     ]
    }
   ],
   "source": [
    "# Create the image_loader\n",
    "tactile_loader = data.DataLoader(whole_hand_dset, \n",
    "                                batch_size  = 128, \n",
    "                                shuffle     = True, \n",
    "                                num_workers = 8,\n",
    "                                pin_memory  = True)\n",
    "whole_hand_mean, whole_hand_std = get_image_stats(\n",
    "    len_whole_hand_dset, \n",
    "    tactile_loader,\n",
    "    img_size = 16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0932e03a-4d34-4791-8804-035ff2c14e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_max(image_loader, channel_num):\n",
    "    min_val = torch.ones(channel_num) * 1000\n",
    "    max_val = -torch.ones(channel_num) * 1000\n",
    "    for inputs in tqdm(image_loader):\n",
    "        curr_min_val = torch.Tensor([inputs[:,i,:,:].min() for i in range(channel_num)]).unsqueeze(1).unsqueeze(1)\n",
    "        curr_max_val = torch.Tensor([inputs[:,i,:,:].max() for i in range(channel_num)]).unsqueeze(1).unsqueeze(1)\n",
    "        \n",
    "        for i in range(channel_num):\n",
    "            min_val[i] = min(min_val[i], curr_min_val[i])\n",
    "            max_val[i] = max(max_val[i], curr_max_val[i])\n",
    "    \n",
    "    print(f'min_val: {min_val}')\n",
    "    print(f'max_val: {max_val}')\n",
    "    \n",
    "    return min_val, max_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5bde34a-ef91-4757-ab1a-2174936273b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_hand_mean, whole_hand_std = torch.Tensor([ 0.1218,  0.7076, 12.5459]), torch.Tensor([168.1889,  98.6576, 104.8352])\n",
    "whole_hand_dset = TactileBYOLDataset(\n",
    "    data_path = PLAY_DATA_DIR,\n",
    "    tactile_information_type = 'whole_hand', # It could be either one of - stacked, whole_hand, single_sensor\n",
    "    img_size = 16,\n",
    "    mean_std=[whole_hand_mean, whole_hand_std], # This is a general stats for all tactile information\n",
    "    min_max=None # Minimum and maximum of the tactile dataset - if given none these values should be found by using this dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c71c556-8ff4-4390-aa67-b6909a749453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████| 3481/3481 [00:06<00:00, 508.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_val: tensor([-107.0494, -172.3380,  -84.2302])\n",
      "max_val: tensor([112.6702, 142.1870, 130.8831])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the image_loader and find min and max \n",
    "tactile_loader = data.DataLoader(whole_hand_dset, \n",
    "                                batch_size  = 128, \n",
    "                                shuffle     = True, \n",
    "                                num_workers = 8,\n",
    "                                pin_memory  = True)\n",
    "whole_hand_min, whole_hand_max = find_min_max(\n",
    "    tactile_loader,\n",
    "    channel_num = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c6f5a4a-3548-453f-b7d0-c2106e6103e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_hand_mean = torch.Tensor([ 0.1218,  0.7076, 12.5459])\n",
    "whole_hand_std = torch.Tensor([168.1889,  98.6576, 104.8352])\n",
    "whole_hand_min = torch.Tensor([-107.0494, -172.3380,  -84.2302]).unsqueeze(1).unsqueeze(1) # Make it 3 dimensional\n",
    "whole_hand_max = torch.Tensor([112.6702, 142.1870, 130.8831]).unsqueeze(1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdbcab7b-af87-438e-8b15-f365ce25ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_hand_dset = TactileBYOLDataset(\n",
    "    data_path = PLAY_DATA_DIR,\n",
    "    tactile_information_type = 'whole_hand', # It could be either one of - stacked, whole_hand, single_sensor\n",
    "    img_size = 16,\n",
    "    mean_std=[whole_hand_mean, whole_hand_std], # This is a general stats for all tactile information\n",
    "    min_max=[whole_hand_min, whole_hand_max] # Minimum and maximum of the tactile dataset - if given none these values should be found by using this dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b328d3e8-4124-4c5d-a3a1-71d6f6d3ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tactile_learning.utils.visualization import plot_tactile_sensor\n",
    "def plot_tactile_image(ax, tactile_image): # Tactile image should be tensor\n",
    "    # img = (tactile_image - tactile_min) / (tactile_max - tactile_min)\n",
    "    npimg = tactile_image.numpy()\n",
    "    ax.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d083ebe-e7fd-4897-8a2a-3068f4e9331a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3691) tensor(0.8181)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1400x700 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tactile_loader = data.DataLoader(whole_hand_dset, \n",
    "                                batch_size  = 128, \n",
    "                                shuffle     = True, \n",
    "                                num_workers = 8,\n",
    "                                pin_memory  = True)\n",
    "for batch_idx, inputs in enumerate(tactile_loader):\n",
    "    fig = plt.figure(figsize = (14, 7))\n",
    "    print(inputs.min(), inputs.max())\n",
    "    for i in range(8):\n",
    "        ax = fig.add_subplot(2, 4, i + 1, xticks = [], yticks = [])  \n",
    "        plot_tactile_image(ax, inputs[i])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92862419-c65f-4fc8-8504-692863a8d412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████| 3481/3481 [00:09<00:00, 369.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor([0.4872, 0.5479, 0.3916])\n",
      "std:  tensor([0.0046, 0.0022, 0.0046])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now find the mean and std again through the normalized images\n",
    "tactile_loader = data.DataLoader(whole_hand_dset, \n",
    "                                batch_size  = 128, \n",
    "                                shuffle     = True, \n",
    "                                num_workers = 8,\n",
    "                                pin_memory  = True)\n",
    "byol_whole_hand_mean, byol_whole_hand_std = get_image_stats(\n",
    "    len(whole_hand_dset), \n",
    "    tactile_loader,\n",
    "    img_size = 16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efab990-7263-4e7e-a091-1fd1f775ae0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
