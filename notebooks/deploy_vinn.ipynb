{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import h5py\n",
    "import numpy as np \n",
    "import os\n",
    "import pickle\n",
    "import torch \n",
    "\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "# from tactile_learning.deployment.load_models import load_model\n",
    "from tactile_learning.datasets.tactile import TactileImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper script to load models\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data as data \n",
    "import torchvision.transforms as T\n",
    "\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm \n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from holobot.robot.allegro.allegro_kdl import AllegroKDL\n",
    "from tactile_learning.models.custom import TactileJointLinear, TactileImageEncoder\n",
    "\n",
    "def load_model(cfg, device, model_path):\n",
    "    # Initialize the model\n",
    "    if cfg.agent_type == 'bc':\n",
    "        model = TactileJointLinear(\n",
    "            input_dim=cfg.tactile_info_dim + cfg.joint_pos_dim,\n",
    "            output_dim=cfg.joint_pos_dim,\n",
    "            hidden_dim=cfg.hidden_dim\n",
    "        )\n",
    "    elif cfg.agent_type == 'byol': # load the encoder\n",
    "        model = TactileImageEncoder(\n",
    "            in_channels=cfg.encoder.in_channels,\n",
    "            out_dim=cfg.encoder.out_dim\n",
    "        )\n",
    "    # print('model: {}'.format(model))\n",
    "    state_dict = torch.load(model_path)\n",
    "    \n",
    "    # Modify the state dict accordingly - this is needed when multi GPU saving was done\n",
    "    new_state_dict = modify_multi_gpu_state_dict(state_dict)\n",
    "    \n",
    "    if cfg.agent_type == 'byol':\n",
    "        new_state_dict = modify_byol_state_dict(new_state_dict)\n",
    "\n",
    "    # Load the new state dict to the model \n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "    # Turn it into DDP - it was saved that way \n",
    "    model = DDP(model.to(device), device_ids=[0])\n",
    "\n",
    "    return model\n",
    "\n",
    "def modify_multi_gpu_state_dict(state_dict):\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:]\n",
    "        new_state_dict[name] = v \n",
    "    return new_state_dict\n",
    "\n",
    "def modify_byol_state_dict(state_dict):\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        if 'encoder.net' in k:\n",
    "            name = k[12:] # Everything after encoder.net\n",
    "            new_state_dict[name] = v\n",
    "    # print(new_state_dict['encoder.net'])\n",
    "    return new_state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to deploy VINN with a saved encoder\n",
    "\n",
    "# Get the out dir\n",
    "out_dir = '/home/irmak/Workspace/tactile-learning/tactile_learning/out/2022.11.30/12-17_byol_bs_2048_joystick'\n",
    "\n",
    "class DeployVINN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_dir,\n",
    "        sensor_indices = (3,7),\n",
    "        allegro_finger_indices = (0,1)\n",
    "    ):\n",
    "        # os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "        # os.environ[\"MASTER_PORT\"] = \"29505\"\n",
    "\n",
    "        # torch.distributed.init_process_group(backend='gloo', rank=0, world_size=1)\n",
    "        torch.cuda.set_device(0)\n",
    "\n",
    "        self.sensor_indices = sensor_indices \n",
    "        self.allegro_finger_indices = [j for i in allegro_finger_indices for j in range(i*3,(i+1)*3)]\n",
    "\n",
    "        self.device = torch.device('cuda:0')\n",
    "        self.cfg = OmegaConf.load(os.path.join(out_dir, '.hydra/config.yaml'))\n",
    "        self.data_path = self.cfg.data_dir\n",
    "        model_path = os.path.join(out_dir, 'models/byol_encoder.pt')\n",
    "\n",
    "        self.encoder = load_model(self.cfg, self.device, model_path)\n",
    "        self.encoder.eval() \n",
    "\n",
    "        self.resize_transform = T.Resize((8, 8))\n",
    "\n",
    "        tactile_values, allegro_tip_positions = self._load_data()\n",
    "        self._get_all_representations(tactile_values, allegro_tip_positions)\n",
    "\n",
    "        self.kdl_solver = AllegroKDL()\n",
    "\n",
    "    def _load_data(self):\n",
    "        roots = glob.glob(f'{self.data_path}/demonstration_*')\n",
    "        roots = sorted(roots)\n",
    "\n",
    "        self.tactile_indices = [] \n",
    "        self.allegro_indices = [] \n",
    "        self.allegro_action_indices = [] \n",
    "        self.allegro_actions = [] \n",
    "        tactile_values = [] \n",
    "        allegro_tip_positions = []\n",
    "\n",
    "        for root in roots:\n",
    "            # Load the indices\n",
    "            with open(os.path.join(root, 'tactile_indices.pkl'), 'rb') as f:\n",
    "                self.tactile_indices += pickle.load(f)\n",
    "            with open(os.path.join(root, 'allegro_indices.pkl'), 'rb') as f:\n",
    "                self.allegro_indices += pickle.load(f)\n",
    "            with open(os.path.join(root, 'allegro_action_indices.pkl'), 'rb') as f:\n",
    "                self.allegro_action_indices += pickle.load(f)\n",
    "\n",
    "            # Load the data\n",
    "            with h5py.File(os.path.join(root, 'allegro_fingertip_states.h5'), 'r') as f:\n",
    "                # print(f['positions'][()].shape)\n",
    "                allegro_tip_positions.append(f['positions'][()][:, self.allegro_finger_indices])\n",
    "            with h5py.File(os.path.join(root, 'allegro_commanded_joint_states.h5'), 'r') as f:\n",
    "                self.allegro_actions.append(f['positions'][()]) # Positions are to be learned - since this is a position control\n",
    "            with h5py.File(os.path.join(root, 'touch_sensor_values.h5'), 'r') as f:\n",
    "                tactile_values.append(f['sensor_values'][()][:,self.sensor_indices,:,:])\n",
    "\n",
    "        # print(self.allegro_tip_positions[0].shape, self.tactile_values[0].shape)\n",
    "        return tactile_values, allegro_tip_positions\n",
    "\n",
    "    def _get_tactile_image(self, tactile_value):\n",
    "        tactile_image = torch.FloatTensor(tactile_value)\n",
    "        tactile_image = tactile_image.reshape((\n",
    "            len(self.sensor_indices),  # Total number of sensors - (2,16,3)\n",
    "            4, \n",
    "            4,\n",
    "            -1\n",
    "        ))\n",
    "        # TODO: This will only work for this grid\n",
    "        tactile_image = torch.concat((tactile_image[0], tactile_image[1]), dim=1)\n",
    "        tactile_image = torch.permute(tactile_image, (2,0,1))\n",
    "\n",
    "        return self.resize_transform(tactile_image)\n",
    "\n",
    "    # tactile_values: (2,16,3)\n",
    "    # allegro_tip_positions: (6,) - 3 values for each finger\n",
    "    def _get_one_representation(self, tactile_values, allegro_tip_positions):\n",
    "        # For each tactile value get the tactile image\n",
    "        tactile_image = self._get_tactile_image(tactile_values).unsqueeze(dim=0)\n",
    "        # print('tactile_image.shape: {}'.format(tactile_image.shape))\n",
    "        tactile_repr = self.encoder(tactile_image)\n",
    "        tactile_repr = tactile_repr.detach().cpu().numpy().squeeze() # Remove the axes with dimension 1\n",
    "        # print('tactile_repr.shape: {}'.format(tactile_repr.shape))\n",
    "        # It should be (64,)\n",
    "        return np.concatenate((tactile_repr, allegro_tip_positions), axis=0)\n",
    "\n",
    "\n",
    "    def _get_all_representations(\n",
    "        self,\n",
    "        tactile_values,\n",
    "        allegro_tip_positions\n",
    "    ):  \n",
    "        print('Getting all representations')\n",
    "        pbar = tqdm(total=len(self.tactile_indices))\n",
    "        # For each tactile value and allegro tip position \n",
    "        # get one representation and add it to all representations\n",
    "        repr_dim = self.cfg.encoder.out_dim + len(self.cfg.dataset.allegro_finger_indices) * 3 \n",
    "        # print('repr_dim: {}'.format(repr_dim))\n",
    "        self.all_representations = np.zeros((\n",
    "            len(self.tactile_indices), repr_dim\n",
    "        ))\n",
    "\n",
    "        for index in range(len(self.tactile_indices)):\n",
    "            demo_id, tactile_id = self.tactile_indices[index]\n",
    "            _, allegro_tip_id = self.allegro_action_indices[index]\n",
    "\n",
    "            tactile_value = tactile_values[demo_id][tactile_id] # This should be (2,16,3)\n",
    "            # print('tactile_value.shape: {}'.format(tactile_value.shape))\n",
    "            allegro_tip_position = allegro_tip_positions[demo_id][tactile_id] # This should be (6,)\n",
    "            # print('allegro_tip_position.shape: {}'.format(allegro_tip_position.shape))\n",
    "            representation = self._get_one_representation(\n",
    "                tactile_value, \n",
    "                allegro_tip_position\n",
    "            )\n",
    "            self.all_representations[index, :] = representation[:]\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "    # tactile_values.shape: (16,15,3)\n",
    "    # joint_state.shape: (16)\n",
    "    def get_action(self, tactile_values, joint_state):\n",
    "        # Get the allegro tip positions with kdl solver \n",
    "        fingertip_positions = self.kdl_solver.get_fingertip_coords(joint_state) # - fingertip position.shape: (12)\n",
    "\n",
    "        # Get the tactile image from the tactile values\n",
    "        curr_tactile_values = tactile_values[self.sensor_indices,:,:]\n",
    "        curr_fingertip_position = fingertip_positions[self.allegro_finger_indices]\n",
    "\n",
    "        print('curr_tactile_values.shape: {}, curr_fingertip_position.shape: {}'.format(\n",
    "            curr_tactile_values.shape, curr_fingertip_position.shape\n",
    "        ))\n",
    "\n",
    "        assert curr_tactile_values.shape == (2,16,3) and curr_fingertip_position.shape == (6,)\n",
    "\n",
    "        # Get the representation with the given tactile value\n",
    "        curr_representation = self._get_one_representation(\n",
    "            curr_tactile_values, \n",
    "            curr_fingertip_position\n",
    "        )\n",
    "\n",
    "        nn_id = self._get_knn_idxs(curr_representation, k=0)\n",
    "        print('nn_id: {}'.format(nn_id))\n",
    "\n",
    "        # Get the applied action at that id\n",
    "        demo_id, action_id = self.allegro_action_indices[nn_id[0]]\n",
    "        nn_action = self.allegro_actions[demo_id][action_id]\n",
    "\n",
    "        print('nn_action: {}'.format(nn_action))\n",
    "\n",
    "        return nn_action\n",
    "\n",
    "    def _get_sorted_idxs(self, representation):\n",
    "        l1_distances = self.all_representations - representation\n",
    "        print('l1_distances.shape: {}'.format(l1_distances.shape))\n",
    "        l2_distances = np.linalg.norm(l1_distances, axis = 1)\n",
    "\n",
    "        sorted_idxs = np.argsort(l2_distances)\n",
    "        return sorted_idxs\n",
    "\n",
    "    def _get_knn_idxs(self, representation, k=0):\n",
    "        sorted_idxs = self._get_sorted_idxs(representation)\n",
    "        \n",
    "        knn_idxs = sorted_idxs[:k+1]\n",
    "        return knn_idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting all representations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52870/52870 [00:27<00:00, 1914.73it/s]\n",
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/ikpy/chain.py:60: UserWarning: Link Base link (index: 0) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\n",
      "  warnings.warn(\"Link {} (index: {}) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\".format(link.name, link_index))\n",
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/ikpy/chain.py:60: UserWarning: Link joint_3.0_tip (index: 5) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\n",
      "  warnings.warn(\"Link {} (index: {}) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\".format(link.name, link_index))\n",
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/ikpy/chain.py:60: UserWarning: Link joint_7.0_tip (index: 5) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\n",
      "  warnings.warn(\"Link {} (index: {}) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\".format(link.name, link_index))\n",
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/ikpy/chain.py:60: UserWarning: Link joint_11.0_tip (index: 5) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\n",
      "  warnings.warn(\"Link {} (index: {}) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\".format(link.name, link_index))\n",
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/ikpy/chain.py:60: UserWarning: Link joint_15.0_tip (index: 5) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\n",
      "  warnings.warn(\"Link {} (index: {}) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\".format(link.name, link_index))\n"
     ]
    }
   ],
   "source": [
    "vinn = DeployVINN(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(15,16,3)\n",
    "y = np.random.rand(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_tactile_values.shape: (2, 16, 3), curr_fingertip_position.shape: (6,)\n",
      "l1_distances.shape: (52870, 70)\n",
      "nn_id: [33830]\n",
      "nn_action: [0.         0.09693163 0.03691709 0.21400803 0.         0.01564443\n",
      " 0.05228145 0.22613142 0.08778772 0.16179268 0.07308225 0.09116157\n",
      " 0.51118785 0.26664537 0.7438539  0.65285134]\n"
     ]
    }
   ],
   "source": [
    "action = vinn.get_action(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(13)\n",
    "x[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tactile_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36852de55b288c46ba617fd48cf310240e4201e2f57004cbdac030fa23152bd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
