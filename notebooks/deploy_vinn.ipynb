{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import h5py\n",
    "import numpy as np \n",
    "import os\n",
    "import pickle\n",
    "import torch \n",
    "\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "# from tactile_learning.deployment.load_models import load_model\n",
    "from tactile_learning.datasets.tactile import TactileImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper script to load models\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data as data \n",
    "import torchvision.transforms as T\n",
    "\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm \n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from holobot.robot.allegro.allegro_kdl import AllegroKDL\n",
    "from tactile_learning.models.custom import TactileJointLinear, TactileImageEncoder\n",
    "\n",
    "def load_model(cfg, device, model_path):\n",
    "    # Initialize the model\n",
    "    if cfg.agent_type == 'bc':\n",
    "        model = TactileJointLinear(\n",
    "            input_dim=cfg.tactile_info_dim + cfg.joint_pos_dim,\n",
    "            output_dim=cfg.joint_pos_dim,\n",
    "            hidden_dim=cfg.hidden_dim\n",
    "        )\n",
    "    elif cfg.agent_type == 'byol': # load the encoder\n",
    "        model = TactileImageEncoder(\n",
    "            in_channels=cfg.encoder.in_channels,\n",
    "            out_dim=cfg.encoder.out_dim\n",
    "        )\n",
    "    # print('model: {}'.format(model))\n",
    "    state_dict = torch.load(model_path)\n",
    "    \n",
    "    # Modify the state dict accordingly - this is needed when multi GPU saving was done\n",
    "    new_state_dict = modify_multi_gpu_state_dict(state_dict)\n",
    "    \n",
    "    if cfg.agent_type == 'byol':\n",
    "        new_state_dict = modify_byol_state_dict(new_state_dict)\n",
    "\n",
    "    # Load the new state dict to the model \n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "    # Turn it into DDP - it was saved that way \n",
    "    model = DDP(model.to(device), device_ids=[0])\n",
    "\n",
    "    return model\n",
    "\n",
    "def modify_multi_gpu_state_dict(state_dict):\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:]\n",
    "        new_state_dict[name] = v \n",
    "    return new_state_dict\n",
    "\n",
    "def modify_byol_state_dict(state_dict):\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        if 'encoder.net' in k:\n",
    "            name = k[12:] # Everything after encoder.net\n",
    "            new_state_dict[name] = v\n",
    "    # print(new_state_dict['encoder.net'])\n",
    "    return new_state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle \n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import tqdm \n",
    "\n",
    "from holobot.robot.allegro.allegro_kdl import AllegroKDL\n",
    "from tactile_learning.deployment.load_models import load_model\n",
    "from tactile_learning.deployment.nn_buffer import NearestNeighborBuffer\n",
    "from tactile_learning.models.knn import KNearestNeighbors\n",
    "from tactile_learning.utils.visualization import dump_camera_image, dump_tactile_state, dump_knn_state\n",
    "from tactile_learning.utils.tactile_image import get_tactile_image\n",
    "\n",
    "class DeployVINN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_dir,\n",
    "        robots = ['allegro', 'kinova'],\n",
    "        sensor_indices = (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14),\n",
    "        allegro_finger_indices = (0,1,2,3),\n",
    "        use_encoder = True,\n",
    "        only_states = False,\n",
    "        nn_buffer_size=100,\n",
    "        nn_k=20\n",
    "    ):\n",
    "        os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "        os.environ[\"MASTER_PORT\"] = \"29505\"\n",
    "\n",
    "        # torch.distributed.init_process_group(backend='gloo', rank=0, world_size=1)\n",
    "        torch.cuda.set_device(0)\n",
    "\n",
    "        self.sensor_indices = sensor_indices \n",
    "        self.allegro_finger_indices = [j for i in allegro_finger_indices for j in range(i*3,(i+1)*3)]\n",
    "        print('self.allegro_finger_indices: {}'.format(self.allegro_finger_indices))\n",
    "        self.only_states = only_states # Check to add tactile info to the representation or not\n",
    "        self.use_encoder = use_encoder\n",
    "        self.robots = robots # Types of robots to be used in the states\n",
    "        print('self.only_states: {}, self.use_encoder: {}'.format(\n",
    "            self.only_states, self.use_encoder\n",
    "        ))\n",
    "\n",
    "        self.device = torch.device('cuda:0')\n",
    "        self.out_dir = out_dir\n",
    "        self.cfg = OmegaConf.load(os.path.join(out_dir, '.hydra/config.yaml'))\n",
    "        self.data_path = self.cfg.data_dir\n",
    "        model_path = os.path.join(out_dir, 'models/byol_encoder.pt')\n",
    "\n",
    "        self.encoder = load_model(self.cfg, self.device, model_path)\n",
    "        self.encoder.eval() \n",
    "\n",
    "        self.resize_transform = T.Resize((self.cfg.tactile_image_size, self.cfg.tactile_image_size))\n",
    "\n",
    "        self._load_data()\n",
    "        self._get_all_representations()\n",
    "        self.state_id = 0 # Increase it with each get_action\n",
    "\n",
    "        self.kdl_solver = AllegroKDL()\n",
    "        self.last_action = None\n",
    "        self.buffer = NearestNeighborBuffer(nn_buffer_size)\n",
    "        self.nn_k = nn_k\n",
    "        self.knn = KNearestNeighbors(\n",
    "            self.all_representations, # Both the input and the output of the nearest neighbors are\n",
    "            self.all_representations\n",
    "        )\n",
    "\n",
    "    def _load_data(self):\n",
    "        roots = glob.glob(f'{self.data_path}/demonstration_*')\n",
    "        roots = sorted(roots)\n",
    "\n",
    "        self.tactile_indices = [] \n",
    "        self.allegro_indices = [] \n",
    "        self.kinova_indices = []\n",
    "        self.allegro_action_indices = [] \n",
    "        self.allegro_actions = [] \n",
    "        self.tactile_values = [] \n",
    "        self.allegro_tip_positions = []\n",
    "        self.kinova_states = []\n",
    "\n",
    "        for root in roots:\n",
    "            # Load the indices\n",
    "            with open(os.path.join(root, 'tactile_indices.pkl'), 'rb') as f:\n",
    "                self.tactile_indices += pickle.load(f)\n",
    "            with open(os.path.join(root, 'allegro_indices.pkl'), 'rb') as f:\n",
    "                self.allegro_indices += pickle.load(f)\n",
    "            with open(os.path.join(root, 'allegro_action_indices.pkl'), 'rb') as f:\n",
    "                self.allegro_action_indices += pickle.load(f)\n",
    "            with open(os.path.join(root, 'kinova_indices.pkl'), 'rb') as f:\n",
    "                self.kinova_indices += pickle.load(f)\n",
    "\n",
    "            # Load the data\n",
    "            with h5py.File(os.path.join(root, 'allegro_fingertip_states.h5'), 'r') as f:\n",
    "                self.allegro_tip_positions.append(f['positions'][()][:, self.allegro_finger_indices])\n",
    "            with h5py.File(os.path.join(root, 'allegro_commanded_joint_states.h5'), 'r') as f:\n",
    "                self.allegro_actions.append(f['positions'][()]) # Positions are to be learned - since this is a position control\n",
    "            with h5py.File(os.path.join(root, 'touch_sensor_values.h5'), 'r') as f:\n",
    "                self.tactile_values.append(f['sensor_values'][()][:,self.sensor_indices,:,:])\n",
    "            with h5py.File(os.path.join(root, 'kinova_cartesian_states.h5'), 'r') as f:\n",
    "                # state = np.concatenate((f['positions'][()], f['orientations'][()]), axis=1) # TODO: Will need to probs change this\n",
    "                # self.kinova_states.append(state)\n",
    "                self.kinova_states.append(f['positions'][()])\n",
    "\n",
    "    def _get_tactile_image(self, tactile_value):\n",
    "        tactile_image = get_tactile_image(tactile_value)\n",
    "        return self.resize_transform(tactile_image)\n",
    "\n",
    "    # tactile_values: (N,16,3) - N: number of sensors\n",
    "    # robot_states: { allegro: allegro_tip_positions: (3*M,) - 3 values for each finger M: number of fingers included,\n",
    "    #                 kinova: kinova_states : (3,) - cartesian position of the arm}\n",
    "    def _get_one_representation(self, tactile_values, robot_states):\n",
    "        states = np.concatenate([robot_states[robot_type] for robot_type in self.robots], axis=0)\n",
    "        assert len(states) == len(self.allegro_finger_indices) + 3, \"len(states) in get_one_representation: {}\".format(len(states))\n",
    "        \n",
    "        # For each tactile value get the tactile image\n",
    "        if self.only_states:\n",
    "            return states\n",
    "\n",
    "        if self.use_encoder:\n",
    "            tactile_image = self._get_tactile_image(tactile_values).unsqueeze(dim=0)\n",
    "            tactile_repr = self.encoder(tactile_image)\n",
    "            tactile_repr = tactile_repr.detach().cpu().numpy().squeeze() # Remove the axes with dimension 1 - shape: (64,)\n",
    "        else:\n",
    "            tactile_repr = tactile_values.flatten() # This will have shape (96,)\n",
    "        return np.concatenate((tactile_repr, states), axis=0)\n",
    "\n",
    "    def _get_all_representations(\n",
    "        self\n",
    "    ):  \n",
    "        print('Getting all representations')\n",
    "        # For each tactile value and allegro tip position \n",
    "        # get one representation and add it to all representations\n",
    "        if self.use_encoder:\n",
    "            repr_dim = self.cfg.encoder.out_dim\n",
    "        else:\n",
    "            repr_dim = len(self.sensor_indices) * 16 * 3\n",
    "        if 'allegro' in self.robots:  repr_dim += len(self.allegro_finger_indices)\n",
    "        if 'kinova' in self.robots: repr_dim += 3\n",
    "\n",
    "        self.all_representations = np.zeros((\n",
    "            len(self.tactile_indices), repr_dim\n",
    "        ))\n",
    "\n",
    "        print('all_representations.shape: {}'.format(self.all_representations.shape))\n",
    "        pbar = tqdm(total=len(self.tactile_indices))\n",
    "\n",
    "        for index in range(len(self.tactile_indices)):\n",
    "            demo_id, tactile_id = self.tactile_indices[index]\n",
    "            _, allegro_tip_id = self.allegro_indices[index]\n",
    "            _, kinova_id = self.kinova_indices[index]\n",
    "\n",
    "            tactile_value = self.tactile_values[demo_id][tactile_id] # This should be (N,16,3)\n",
    "            allegro_tip_position = self.allegro_tip_positions[demo_id][allegro_tip_id] # This should be (M*3,)\n",
    "            kinova_state = self.kinova_states[demo_id][kinova_id]\n",
    "            \n",
    "            robot_states = dict(\n",
    "                allegro = allegro_tip_position,\n",
    "                kinova = kinova_state\n",
    "            )\n",
    "            representation = self._get_one_representation(\n",
    "                tactile_value, \n",
    "                robot_states\n",
    "            )\n",
    "            # Add only tip positions as the representation\n",
    "            self.all_representations[index, :] = representation[:]\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "    # tactile_values.shape: (16,15,3)\n",
    "    # robot_state: {allegro: allegro_joint_state (16,), kinova: kinova_cart_state (7,)}\n",
    "    def get_action(self, tactile_values, recv_robot_state, visualize=False):\n",
    "        # Get the allegro tip positions with kdl solver \n",
    "        allegro_joint_state = recv_robot_state['allegro']\n",
    "        fingertip_positions = self.kdl_solver.get_fingertip_coords(allegro_joint_state) # - fingertip position.shape: (12)\n",
    "        \n",
    "        if 'kinova' in self.robots:\n",
    "            kinova_cart_state = recv_robot_state['kinova']\n",
    "\n",
    "        # Get the tactile image from the tactile values\n",
    "        curr_tactile_values = tactile_values[self.sensor_indices,:,:]\n",
    "        curr_fingertip_position = fingertip_positions[self.allegro_finger_indices]\n",
    "\n",
    "        print('curr_tactile_values.shape: {}, curr_fingertip_position.shape: {}, curr_kinova_state.shape: {}'.format(\n",
    "            curr_tactile_values.shape, curr_fingertip_position.shape, kinova_cart_state.shape\n",
    "        ))\n",
    "\n",
    "        assert curr_tactile_values.shape == (len(self.sensor_indices),16,3) and curr_fingertip_position.shape == (len(self.allegro_finger_indices),)\n",
    "\n",
    "        curr_robot_state = dict(\n",
    "            allegro = curr_fingertip_position,\n",
    "            kinova = kinova_cart_state\n",
    "        )\n",
    "        # Get the representation with the given tactile value\n",
    "        curr_representation = self._get_one_representation(\n",
    "            curr_tactile_values, \n",
    "            curr_robot_state\n",
    "        )\n",
    "\n",
    "        # k = 20\n",
    "        # if self.only_states:\n",
    "        #     only_tip_representation = np.zeros_like(curr_representation)\n",
    "        #     only_tip_representation[-6:] = curr_representation[-6:]\n",
    "        #     nn_idxs = self._get_knn_idxs(only_tip_representation, k=k) # TODO: Fix this\n",
    "        # else:\n",
    "        #     nn_idxs = self._get_knn_idxs(curr_representation, k=k)\n",
    "\n",
    "        _, nn_idxs = self.knn.get_k_nearest_neighbors(curr_representation, k=self.nn_k)\n",
    "        print('nn_idxs: {}'.format(nn_idxs))\n",
    "\n",
    "        # Choose the action with the buffer \n",
    "        id_of_nn = self.buffer.choose(nn_idxs)\n",
    "        nn_id = nn_idxs[id_of_nn]\n",
    "        print('chosen nn_id: {}'.format(nn_id))\n",
    "        demo_id, action_id = self.allegro_action_indices[nn_id] \n",
    "        nn_allegro_action = self.allegro_actions[demo_id][action_id+1] # Get the next commanded action (commanded actions are saved in that timestamp)\n",
    "        nn_action = dict(\n",
    "            allegro = nn_allegro_action\n",
    "        )\n",
    "        \n",
    "        if 'kinova' in self.robots:\n",
    "            _, kinova_id = self.kinova_indices[nn_id] \n",
    "            nn_kinova_action = self.kinova_states[demo_id][kinova_id+1] # Get the next saved kinova_state\n",
    "            print('kinova action shape: {}'.format(nn_kinova_action.shape))\n",
    "            assert nn_kinova_action.shape == (3,), 'kinova_action shape : {} incorrect - it shouldve been (3,)'.format(nn_kinova_action.shape)\n",
    "            kinova_action = np.zeros(6)\n",
    "            kinova_action[:3] = nn_kinova_action - kinova_cart_state # Since we're only doing velocity control\n",
    "            nn_action['kinova'] = kinova_action\n",
    "\n",
    "        # Visualize if given \n",
    "        if visualize: # TODO: This should be fixed for the whole robot\n",
    "            self._visualize_state(\n",
    "                curr_tactile_values, \n",
    "                curr_fingertip_position,\n",
    "                nn_id\n",
    "            )\n",
    "\n",
    "        print('nn_action: {}'.format(nn_action))\n",
    "        self.state_id += 1\n",
    "\n",
    "        return nn_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.allegro_finger_indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "self.only_states: False, self.use_encoder: True\n",
      "Getting all representations\n",
      "all_representations.shape: (72266, 79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72266/72266 [00:43<00:00, 1660.80it/s]\n",
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/ikpy/chain.py:60: UserWarning: Link Base link (index: 0) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\n",
      "  warnings.warn(\"Link {} (index: {}) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\".format(link.name, link_index))\n",
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/ikpy/chain.py:60: UserWarning: Link joint_3.0_tip (index: 5) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\n",
      "  warnings.warn(\"Link {} (index: {}) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\".format(link.name, link_index))\n",
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/ikpy/chain.py:60: UserWarning: Link joint_7.0_tip (index: 5) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\n",
      "  warnings.warn(\"Link {} (index: {}) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\".format(link.name, link_index))\n",
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/ikpy/chain.py:60: UserWarning: Link joint_11.0_tip (index: 5) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\n",
      "  warnings.warn(\"Link {} (index: {}) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\".format(link.name, link_index))\n",
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/ikpy/chain.py:60: UserWarning: Link joint_15.0_tip (index: 5) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\n",
      "  warnings.warn(\"Link {} (index: {}) is of type 'fixed' but set as active in the active_links_mask. In practice, this fixed link doesn't provide any transformation so is as it were inactive\".format(link.name, link_index))\n"
     ]
    }
   ],
   "source": [
    "out_dir = '/home/irmak/Workspace/tactile-learning/tactile_learning/out/2023.01.02/19-29_byol_bs_1028_box_handle_lifting'\n",
    "vinn = DeployVINN(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(15,16,3)\n",
    "allegro_joint_states = np.random.rand(16)\n",
    "kinova_cart_states = np.random.rand(3)\n",
    "robot_state = dict(\n",
    "    allegro = allegro_joint_states,\n",
    "    kinova = kinova_cart_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_tactile_values.shape: (15, 16, 3), curr_fingertip_position.shape: (12,), curr_kinova_state.shape: (3,)\n",
      "nn_idxs: [37896 37897 37895 37894 37901 14827 14829 14828 15566 15567 35652 14832\n",
      " 14831 14830 35659 35657 35658 35661 35660 35654]\n",
      "chosen nn_id: 37896\n",
      "kinova action shape: (3,)\n",
      "nn_action: {'allegro': array([ 0.        , -0.17453294,  0.7853982 ,  0.7853982 ,  0.        ,\n",
      "       -0.17453294,  0.7853982 ,  0.7853982 ,  0.08726646, -0.08726646,\n",
      "        0.87266463,  0.7853982 ,  1.0471976 ,  0.43633232,  0.2617994 ,\n",
      "        0.7853982 ], dtype=float32), 'kinova': array([-1.19047608,  0.01418956, -0.3219771 ,  0.        ,  0.        ,\n",
      "        0.        ])}\n"
     ]
    }
   ],
   "source": [
    "action = vinn.get_action(x, robot_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tactile_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36852de55b288c46ba617fd48cf310240e4201e2f57004cbdac030fa23152bd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
