{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to have simple online RL algorithm running\n",
    "# We will only switch env with the hand environment\n",
    "\n",
    "import gym # Will use simple cartpole with actor critic\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    \"\"\"Buffer to store environment transitions.\"\"\"\n",
    "    def __init__(self, obs_shape, action_shape, capacity, device):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "\n",
    "        # the proprioceptive obs is stored as float32, pixels obs as uint8\n",
    "        obs_dtype = np.float32 if len(obs_shape) == 1 else np.uint8\n",
    "\n",
    "        self.obses = np.empty((capacity, *obs_shape), dtype=obs_dtype)\n",
    "        self.next_obses = np.empty((capacity, *obs_shape), dtype=obs_dtype)\n",
    "        self.actions = np.empty((capacity, *action_shape), dtype=np.float32)\n",
    "        self.rewards = np.empty((capacity, 1), dtype=np.float32)\n",
    "        self.not_dones = np.empty((capacity, 1), dtype=np.float32)\n",
    "        self.not_dones_no_max = np.empty((capacity, 1), dtype=np.float32)\n",
    "\n",
    "        self.idx = 0\n",
    "        self.last_save = 0\n",
    "        self.full = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.capacity if self.full else self.idx\n",
    "\n",
    "    def add(self, obs, action, reward, next_obs, done, done_no_max):\n",
    "        np.copyto(self.obses[self.idx], obs)\n",
    "        np.copyto(self.actions[self.idx], action)\n",
    "        np.copyto(self.rewards[self.idx], reward)\n",
    "        np.copyto(self.next_obses[self.idx], next_obs)\n",
    "        np.copyto(self.not_dones[self.idx], not done)\n",
    "        np.copyto(self.not_dones_no_max[self.idx], not done_no_max)\n",
    "\n",
    "        self.idx = (self.idx + 1) % self.capacity\n",
    "        self.full = self.full or self.idx == 0\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idxs = np.random.randint(0,\n",
    "                                 self.capacity if self.full else self.idx,\n",
    "                                 size=batch_size)\n",
    "\n",
    "        obses = torch.as_tensor(self.obses[idxs], device=self.device).float()\n",
    "        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n",
    "        rewards = torch.as_tensor(self.rewards[idxs], device=self.device)\n",
    "        next_obses = torch.as_tensor(self.next_obses[idxs],\n",
    "                                     device=self.device).float()\n",
    "        not_dones = torch.as_tensor(self.not_dones[idxs], device=self.device)\n",
    "        not_dones_no_max = torch.as_tensor(self.not_dones_no_max[idxs],\n",
    "                                           device=self.device)\n",
    "\n",
    "        return obses, actions, rewards, next_obses, not_dones, not_dones_no_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import distributions as pyd\n",
    "\n",
    "class TanhTransform(pyd.transforms.Transform):\n",
    "    domain = pyd.constraints.real\n",
    "    codomain = pyd.constraints.interval(-1.0, 1.0)\n",
    "    bijective = True\n",
    "    sign = +1\n",
    "\n",
    "    def __init__(self, cache_size=1):\n",
    "        super().__init__(cache_size=cache_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def atanh(x):\n",
    "        return 0.5 * (x.log1p() - (-x).log1p())\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, TanhTransform)\n",
    "\n",
    "    def _call(self, x):\n",
    "        return x.tanh()\n",
    "\n",
    "    def _inverse(self, y):\n",
    "        # We do not clamp to the boundary here as it may degrade the performance of certain algorithms.\n",
    "        # one should use `cache_size=1` instead\n",
    "        return self.atanh(y)\n",
    "\n",
    "    def log_abs_det_jacobian(self, x, y):\n",
    "        # We use a formula that is more numerically stable, see details in the following link\n",
    "        # https://github.com/tensorflow/probability/commit/ef6bb176e0ebd1cf6e25c6b5cecdd2428c22963f#diff-e120f70e92e6741bca649f04fcd907b7\n",
    "        return 2. * (math.log(2.) - x - F.softplus(-2. * x))\n",
    "\n",
    "class SquashedNormal(pyd.transformed_distribution.TransformedDistribution):\n",
    "    def __init__(self, loc, scale):\n",
    "        self.loc = loc\n",
    "        self.scale = scale\n",
    "\n",
    "        self.base_dist = pyd.Normal(loc, scale)\n",
    "        transforms = [TanhTransform()]\n",
    "        super().__init__(self.base_dist, transforms)\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        mu = self.loc\n",
    "        for tr in self.transforms:\n",
    "            mu = tr(mu)\n",
    "        return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tactile_learning.models.utils as utils\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"torch.distributions implementation of an diagonal Gaussian policy.\"\"\"\n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=1024, hidden_depth=2,\n",
    "                 log_std_bounds=[-5,2]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.log_std_bounds = log_std_bounds\n",
    "        self.trunk = utils.mlp(obs_dim, hidden_dim, 2 * action_dim,\n",
    "                               hidden_depth)\n",
    "\n",
    "        self.outputs = dict()\n",
    "        self.apply(utils.weight_init)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        mu, log_std = self.trunk(obs).chunk(2, dim=-1)\n",
    "\n",
    "        # constrain log_std inside [log_std_min, log_std_max]\n",
    "        log_std = torch.tanh(log_std)\n",
    "        log_std_min, log_std_max = self.log_std_bounds\n",
    "        log_std = log_std_min + 0.5 * (log_std_max - log_std_min) * (log_std +\n",
    "                                                                     1)\n",
    "\n",
    "        std = log_std.exp()\n",
    "\n",
    "        self.outputs['mu'] = mu\n",
    "        self.outputs['std'] = std\n",
    "\n",
    "        dist = SquashedNormal(mu, std)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic network, employes double Q-learning.\"\"\"\n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=1024, hidden_depth=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Q1 = utils.mlp(obs_dim + action_dim, hidden_dim, 1, hidden_depth)\n",
    "        self.Q2 = utils.mlp(obs_dim + action_dim, hidden_dim, 1, hidden_depth)\n",
    "\n",
    "        self.outputs = dict()\n",
    "        self.apply(utils.weight_init)\n",
    "\n",
    "    def forward(self, obs, action):\n",
    "        assert obs.size(0) == action.size(0)\n",
    "\n",
    "        obs_action = torch.cat([obs, action], dim=-1)\n",
    "        q1 = self.Q1(obs_action)\n",
    "        q2 = self.Q2(obs_action)\n",
    "\n",
    "        self.outputs['q1'] = q1\n",
    "        self.outputs['q2'] = q2\n",
    "\n",
    "        return q1, q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class Agent(object):\n",
    "    def reset(self):\n",
    "        \"\"\"For state-full agents this function performs reseting at the beginning of each episode.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def train(self, training=True):\n",
    "        \"\"\"Sets the agent in either training or evaluation mode.\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def update(self, replay_buffer, logger, step):\n",
    "        \"\"\"Main function of the agent that performs learning.\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def act(self, obs, sample=False):\n",
    "        \"\"\"Issues an action given an observation.\"\"\"\n",
    "\n",
    "class SACAgent(Agent):\n",
    "    \"\"\"SAC algorithm.\"\"\"\n",
    "    def __init__(self, obs_dim, action_dim, action_range, device, \n",
    "                 discount = 0.99, # TODO: Add these to hydra configs\n",
    "                 init_temperature = 0.1,\n",
    "                 alpha_lr = 1e-4,\n",
    "                 alpha_betas = [0.9, 0.999],\n",
    "                 actor_lr = 1e-4,\n",
    "                 actor_betas = [0.9, 0.999],\n",
    "                 actor_update_frequency = 1,\n",
    "                 critic_lr = 1e-4,\n",
    "                 critic_betas = [0.9, 0.999],\n",
    "                 critic_tau = 0.005,\n",
    "                 critic_target_update_frequency = 2,\n",
    "                 batch_size = 1024,\n",
    "                 learnable_temperature = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.action_range = action_range\n",
    "        self.device = torch.device(device)\n",
    "        self.discount = discount\n",
    "        self.critic_tau = critic_tau\n",
    "        self.actor_update_frequency = actor_update_frequency\n",
    "        self.critic_target_update_frequency = critic_target_update_frequency\n",
    "        self.batch_size = batch_size\n",
    "        self.learnable_temperature = learnable_temperature\n",
    "\n",
    "\n",
    "        self.critic = Critic(obs_dim = obs_dim, action_dim = action_dim).to(self.device)\n",
    "        self.critic_target = Critic(obs_dim = obs_dim, action_dim = action_dim ).to(self.device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        # self.critic = hydra.utils.instantiate(critic_cfg).to(self.device)\n",
    "        # self.critic_target = hydra.utils.instantiate(critic_cfg).to(\n",
    "        #     self.device)\n",
    "        # self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        self.actor = Actor(obs_dim = obs_dim, action_dim = action_dim).to(self.device)\n",
    "        # self.actor = hydra.utils.instantiate(actor_cfg).to(self.device)\n",
    "\n",
    "        self.log_alpha = torch.tensor(np.log(init_temperature)).to(self.device)\n",
    "        self.log_alpha.requires_grad = True\n",
    "        # set target entropy to -|A|\n",
    "        self.target_entropy = -action_dim\n",
    "\n",
    "        # optimizers\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                lr=actor_lr,\n",
    "                                                betas=actor_betas)\n",
    "\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n",
    "                                                 lr=critic_lr,\n",
    "                                                 betas=critic_betas)\n",
    "\n",
    "        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha],\n",
    "                                                    lr=alpha_lr,\n",
    "                                                    betas=alpha_betas)\n",
    "\n",
    "        self.train()\n",
    "        self.critic_target.train()\n",
    "\n",
    "    def train(self, training=True):\n",
    "        self.training = training\n",
    "        self.actor.train(training)\n",
    "        self.critic.train(training)\n",
    "\n",
    "    @property\n",
    "    def alpha(self):\n",
    "        return self.log_alpha.exp()\n",
    "\n",
    "    def act(self, obs, sample=False):\n",
    "        obs = torch.FloatTensor(obs).to(self.device)\n",
    "        obs = obs.unsqueeze(0)\n",
    "        dist = self.actor(obs)\n",
    "        action = dist.sample() if sample else dist.mean\n",
    "        action = action.clamp(*self.action_range)\n",
    "        assert action.ndim == 2 and action.shape[0] == 1\n",
    "        return utils.to_np(action[0])\n",
    "\n",
    "    def update_critic(self, obs, action, reward, next_obs, not_done, logger,\n",
    "                      step):\n",
    "        dist = self.actor(next_obs)\n",
    "        next_action = dist.rsample()\n",
    "        log_prob = dist.log_prob(next_action).sum(-1, keepdim=True)\n",
    "        target_Q1, target_Q2 = self.critic_target(next_obs, next_action)\n",
    "        target_V = torch.min(target_Q1,\n",
    "                             target_Q2) - self.alpha.detach() * log_prob\n",
    "        target_Q = reward + (not_done * self.discount * target_V)\n",
    "        target_Q = target_Q.detach()\n",
    "\n",
    "        # get current Q estimates\n",
    "        current_Q1, current_Q2 = self.critic(obs, action)\n",
    "        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(\n",
    "            current_Q2, target_Q)\n",
    "        logger.log({'step': step,\n",
    "                    'train_critic/loss': critic_loss})\n",
    "\n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # self.critic.log(logger, step)\n",
    "\n",
    "    def update_actor_and_alpha(self, obs, logger, step):\n",
    "        dist = self.actor(obs)\n",
    "        action = dist.rsample()\n",
    "        log_prob = dist.log_prob(action).sum(-1, keepdim=True)\n",
    "        actor_Q1, actor_Q2 = self.critic(obs, action)\n",
    "\n",
    "        actor_Q = torch.min(actor_Q1, actor_Q2)\n",
    "        actor_loss = (self.alpha.detach() * log_prob - actor_Q).mean()\n",
    "\n",
    "        logger.log({'step': step,\n",
    "                    'train_actor/loss': actor_loss})\n",
    "        logger.log({'step': step,\n",
    "                    'train_actor/target_entropy': self.target_entropy})\n",
    "        logger.log({'step': step,\n",
    "                    'train_actor/entropy': -log_prob.mean()})\n",
    "\n",
    "        # optimize the actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # self.actor.log(logger, step)\n",
    "\n",
    "        if self.learnable_temperature:\n",
    "            self.log_alpha_optimizer.zero_grad()\n",
    "            alpha_loss = (self.alpha *\n",
    "                          (-log_prob - self.target_entropy).detach()).mean()\n",
    "            logger.log({'step': step,\n",
    "                        'train_alpha/loss': alpha_loss})\n",
    "            logger.log({'step': step,\n",
    "                        'train_alpha/value': self.alpha})\n",
    "            alpha_loss.backward()\n",
    "            self.log_alpha_optimizer.step()\n",
    "\n",
    "    def update(self, replay_buffer, logger, step):\n",
    "        obs, action, reward, next_obs, not_done, not_done_no_max = replay_buffer.sample(\n",
    "            self.batch_size)\n",
    "\n",
    "        logger.log({'step': step,\n",
    "                    'train/batch_reward': reward.mean()})\n",
    "\n",
    "        self.update_critic(obs, action, reward, next_obs, not_done_no_max,\n",
    "                           logger, step)\n",
    "\n",
    "        if step % self.actor_update_frequency == 0:\n",
    "            self.update_actor_and_alpha(obs, logger, step)\n",
    "\n",
    "        if step % self.critic_target_update_frequency == 0:\n",
    "            utils.soft_update_params(self.critic, self.critic_target,\n",
    "                                     self.critic_tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "class Logger:\n",
    "    def __init__(self, exp_name:str, out_dir:str) -> None:\n",
    "        # Initialize the wandb experiment\n",
    "        self.wandb_logger = wandb.init(project=\"tactile_learning_rl\", \n",
    "                                       name=exp_name, # Config is not added yet\n",
    "                                       settings=wandb.Settings(start_method=\"thread\"))\n",
    "\n",
    "        # self.tb_logger = SummaryWriter(out_dir)\n",
    "        self.logger_file = os.path.join(out_dir, 'train.log')\n",
    "\n",
    "    def log(self, msg):\n",
    "        if type(msg) is dict:\n",
    "            self.wandb_logger.log(msg)\n",
    "\n",
    "        with open(self.logger_file, 'a') as f:\n",
    "            f.write('{}\\n'.format(msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(): \n",
    "    env = gym.make('CartPole-v1')\n",
    "    action_dim = 1 \n",
    "    obs_dim = 4\n",
    "\n",
    "    return env, action_dim, obs_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env, action_dim, obs_dim = make_env()\n",
    "returns = env.reset()\n",
    "# print(returns)\n",
    "print(env.action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.01915576,  0.15229334,  0.01897914, -0.2677058 ], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Workspace(object):\n",
    "    def __init__(self, cfg):\n",
    "        self.work_dir = os.getcwd()\n",
    "        print(f'workspace: {self.work_dir}')\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.logger = Logger(exp_name = )\n",
    "\n",
    "        utils.set_seed_everywhere(cfg.seed)\n",
    "        self.device = torch.device(cfg.device)\n",
    "        self.env, self.action_dim, self.obs_dim = make_env()\n",
    "\n",
    "        cfg.agent.params.obs_dim = self.env.observation_space.shape[0]\n",
    "        cfg.agent.params.action_dim = self.env.action_space.shape[0]\n",
    "        cfg.agent.params.action_range = [\n",
    "            float(self.env.action_space.low.min()),\n",
    "            float(self.env.action_space.high.max())\n",
    "        ]\n",
    "        self.agent = SACAgent(obs_dim=self.obs_dim, action_dim=self.action_dim)\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(self.env.observation_space.shape,\n",
    "                                          self.env.action_space.shape,\n",
    "                                          int(cfg.replay_buffer_capacity),\n",
    "                                          self.device)\n",
    "        self.step = 0\n",
    "\n",
    "    def evaluate(self):\n",
    "        average_episode_reward = 0\n",
    "        for episode in range(self.cfg.num_eval_episodes):\n",
    "            obs = self.env.reset()\n",
    "            self.agent.reset()\n",
    "            self.video_recorder.init(enabled=(episode == 0))\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                with utils.eval_mode(self.agent):\n",
    "                    action = self.agent.act(obs, sample=False)\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                self.video_recorder.record(self.env)\n",
    "                episode_reward += reward\n",
    "\n",
    "            average_episode_reward += episode_reward\n",
    "            self.video_recorder.save(f'{self.step}.mp4')\n",
    "        average_episode_reward /= self.cfg.num_eval_episodes\n",
    "        self.logger.log('eval/episode_reward', average_episode_reward,\n",
    "                        self.step)\n",
    "        self.logger.dump(self.step)\n",
    "\n",
    "    def run(self):\n",
    "        episode, episode_reward, done = 0, 0, True\n",
    "        start_time = time.time()\n",
    "        while self.step < self.cfg.num_train_steps:\n",
    "            if done:\n",
    "                if self.step > 0:\n",
    "                    self.logger.log('train/duration',\n",
    "                                    time.time() - start_time, self.step)\n",
    "                    start_time = time.time()\n",
    "                    self.logger.dump(\n",
    "                        self.step, save=(self.step > self.cfg.num_seed_steps))\n",
    "\n",
    "                # evaluate agent periodically\n",
    "                if self.step > 0 and self.step % self.cfg.eval_frequency == 0:\n",
    "                    self.logger.log('eval/episode', episode, self.step)\n",
    "                    self.evaluate()\n",
    "\n",
    "                self.logger.log('train/episode_reward', episode_reward,\n",
    "                                self.step)\n",
    "\n",
    "                obs = self.env.reset()\n",
    "                self.agent.reset()\n",
    "                done = False\n",
    "                episode_reward = 0\n",
    "                episode_step = 0\n",
    "                episode += 1\n",
    "\n",
    "                self.logger.log('train/episode', episode, self.step)\n",
    "\n",
    "            # sample action for data collection\n",
    "            if self.step < self.cfg.num_seed_steps:\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                with utils.eval_mode(self.agent):\n",
    "                    action = self.agent.act(obs, sample=True)\n",
    "\n",
    "            # run training update\n",
    "            if self.step >= self.cfg.num_seed_steps:\n",
    "                self.agent.update(self.replay_buffer, self.logger, self.step)\n",
    "\n",
    "            next_obs, reward, done, _ = self.env.step(action)\n",
    "\n",
    "            # allow infinite bootstrap\n",
    "            done = float(done)\n",
    "            done_no_max = 0 if episode_step + 1 == self.env._max_episode_steps else done\n",
    "            episode_reward += reward\n",
    "\n",
    "            self.replay_buffer.add(obs, action, reward, next_obs, done,\n",
    "                                   done_no_max)\n",
    "\n",
    "            obs = next_obs\n",
    "            episode_step += 1\n",
    "            self.step += 1\n",
    "\n",
    "\n",
    "@hydra.main(config_path='config/train.yaml', strict=True)\n",
    "def main(cfg):\n",
    "    workspace = Workspace(cfg)\n",
    "    workspace.run()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tactile_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36852de55b288c46ba617fd48cf310240e4201e2f57004cbdac030fa23152bd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
