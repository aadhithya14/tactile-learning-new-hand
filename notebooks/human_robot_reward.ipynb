{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to get rewards from the human demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import hydra\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from tactile_learning.models import *\n",
    "from tactile_learning.utils import *\n",
    "from tactile_learning.tactile_data import *\n",
    "from holobot.robot.allegro.allegro_kdl import AllegroKDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the human demo\n",
    "def load_human_demo(data_path, demo_id, view_num, image_transform=None): # This will have the exact same demo\n",
    "    human_demos = []\n",
    "    image_obs = [] \n",
    "    old_demo_id = -1\n",
    "\n",
    "    if image_transform is None: \n",
    "        def viewed_crop_transform(image):\n",
    "            return crop_transform(image, camera_view=view_num)\n",
    "        image_transform = T.Compose([\n",
    "            T.Resize((480,640)),\n",
    "            T.Lambda(viewed_crop_transform),\n",
    "            T.Resize(480),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(VISION_IMAGE_MEANS, VISION_IMAGE_STDS), \n",
    "        ])\n",
    "\n",
    "    roots = glob.glob(f'{data_path}/demonstration_*')\n",
    "    roots = sorted(roots)\n",
    "    image_root = roots[demo_id]\n",
    "\n",
    "    image_ids = glob.glob(f'{image_root}/cam_{view_num}_rgb_images/*')\n",
    "    image_ids = sorted([int(image_id.split('_')[-1].split('.')[0]) for image_id in image_ids])\n",
    "    print('image_ids: {}'.format(image_ids))\n",
    "    for image_id in image_ids:\n",
    "        image = load_dataset_image( # We only have images in human demonstrations\n",
    "            data_path = data_path, \n",
    "            demo_id = demo_id, \n",
    "            image_id = image_id,\n",
    "            view_num = view_num,\n",
    "            transform = image_transform\n",
    "        )\n",
    "        image_obs.append(image)\n",
    "\n",
    "    human_demos.append(dict(\n",
    "        image_obs = torch.stack(image_obs, 0), \n",
    "    ))\n",
    "\n",
    "    return human_demos\n",
    "\n",
    "    # for step_id in range(len(data['image']['indices'])): \n",
    "    #     demo_id, _ = data['tactile']['indices'][step_id]\n",
    "    #     if (demo_id != old_demo_id and step_id > 0) or (step_id == len(data['image']['indices'])-1): # NOTE: We are losing the last frame of the last expert\n",
    "    #         expert_demos.append(dict(\n",
    "    #             image_obs = torch.stack(image_obs, 0), \n",
    "    #         ))\n",
    "    #         image_obs = [] \n",
    "\n",
    "    #     _, image_id = data['image']['indices'][step_id]\n",
    "    #     image = load_dataset_image( # We only have images in human demonstrations\n",
    "    #         data_path = data_path, \n",
    "    #         demo_id = demo_id, \n",
    "    #         image_id = image_id,\n",
    "    #         view_num = view_num,\n",
    "    #         transform = image_transform\n",
    "    #     )\n",
    "    #     image_obs.append(image)\n",
    "\n",
    "\n",
    "    #     old_demo_id = demo_id\n",
    "\n",
    "    # return expert_demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_ids: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "human_demo = load_human_demo(\n",
    "    data_path = '/home/irmak/Workspace/Holo-Bot/extracted_data/human_bowl_picking',\n",
    "    demo_id = 1,\n",
    "    view_num = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the episode demos\n",
    "from tactile_learning.utils import load_all_episodes, load_episode_demos\n",
    "all_episodes = load_all_episodes(\n",
    "    roots = cfg.episode_roots, # Either one of them will be none\n",
    "    root_path = cfg.episode_root_path\n",
    ")\n",
    "episode_demos = load_episode_demos(\n",
    "    all_episodes= all_episodes,\n",
    "    image_transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(VISION_IMAGE_MEANS, VISION_IMAGE_STDS)\n",
    "    ])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tactile_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
