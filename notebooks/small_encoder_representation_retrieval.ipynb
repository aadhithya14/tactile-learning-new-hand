{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - retrieve a representation by using the single encoders\n",
    "\n",
    "import glob\n",
    "import h5py\n",
    "import hydra\n",
    "import mmap\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import torch.utils.data as data \n",
    "\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm \n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from holobot.robot.allegro.allegro_kdl import AllegroKDL\n",
    "\n",
    "from tactile_learning.utils.constants import *\n",
    "from tactile_learning.models.custom import *\n",
    "from tactile_learning.datasets.tactile_vision import *\n",
    "from tactile_learning.deployment.load_models import * \n",
    "from tactile_learning.deployment.nn_buffer import NearestNeighborBuffer\n",
    "from tactile_learning.models.knn import KNearestNeighbors, ScaledKNearestNeighbors\n",
    "from tactile_learning.utils.visualization import *\n",
    "from tactile_learning.utils.tactile_image import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ROOTS = ['/home/irmak/Workspace/Holo-Bot/deployment_data/box_handle_lifting/demonstration_image_tactile_kinova_scaled_knn_3'] # TODO - change this to knn_4\n",
    "REPR_DIR = '/home/irmak/Workspace/Holo-Bot/extracted_data/box_handle_lifting/box_location_changing/training'\n",
    "REPR_ROOTS = sorted(glob.glob(f'{REPR_DIR}/demonstration_*'))\n",
    "\n",
    "SINGLE_SENSOR_CNN_OUT_DIR = '/home/irmak/Workspace/tactile-learning/tactile_learning/out/2023.01.13/18-43_tactile_byol_bs_1028_tactile_play_data_single_sensor_encoder'\n",
    "SINGLE_SENSOR_LINEAR_OUT_DIR = '/home/irmak/Workspace/tactile-learning/tactile_learning/out/2023.01.13/18-16_tactile_linear_byol_bs_1028_tactile_play_data_linear_encoder'\n",
    "IMAGE_ENCODER_OUT_DIR = '/home/irmak/Workspace/tactile-learning/tactile_learning/out/2023.01.10/20-22_image_byol_bs_32_box_handle_lifting/box_location_changing/training'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "trying to initialize the default process group twice!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mMASTER_ADDR\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlocalhost\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mMASTER_PORT\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m29506\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m torch\u001b[39m.\u001b[39;49mdistributed\u001b[39m.\u001b[39;49minit_process_group(backend\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgloo\u001b[39;49m\u001b[39m'\u001b[39;49m, rank\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, world_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mset_device(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:563\u001b[0m, in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    559\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected timeout argument to be of type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdatetime.timedelta\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m     )\n\u001b[1;32m    562\u001b[0m \u001b[39mif\u001b[39;00m GroupMember\u001b[39m.\u001b[39mWORLD \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtrying to initialize the default process group \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtwice!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    565\u001b[0m \u001b[39massert\u001b[39;00m (store \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    566\u001b[0m     init_method \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    567\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mCannot specify both init_method and store.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m \u001b[39mif\u001b[39;00m store \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: trying to initialize the default process group twice!"
     ]
    }
   ],
   "source": [
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29506\"\n",
    "\n",
    "torch.distributed.init_process_group(backend='gloo', rank=0, world_size=1)\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_encoder_info(device, out_dir, encoder_type='tactile'): # encoder_type: either image or tactile\n",
    "    # print('out_dir: {}'.format(out_dir))\n",
    "    cfg = OmegaConf.load(os.path.join(out_dir, '.hydra/config.yaml'))\n",
    "    model_path = os.path.join(out_dir, 'models/byol_encoder_best.pt')\n",
    "    encoder = load_model(cfg, device, model_path)\n",
    "    encoder.eval() \n",
    "    if encoder_type == 'tactile':\n",
    "        transform = T.Compose([\n",
    "            T.Resize((4,4)),\n",
    "            T.Normalize(TACTILE_IMAGE_MEANS, TACTILE_IMAGE_STDS),\n",
    "        ])\n",
    "    elif encoder_type == 'image':\n",
    "        transform = T.Compose([\n",
    "            T.Resize((480,640)),\n",
    "            T.Lambda(crop_transform),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(VISION_IMAGE_MEANS, VISION_IMAGE_STDS),\n",
    "        ]) \n",
    "\n",
    "    return cfg, encoder, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg.learner_type: tactile_byol\n",
      "cfg.learner_type: image_byol\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "tactile_cfg, tactile_single_cnn, tactile_transform = init_encoder_info(device, SINGLE_SENSOR_CNN_OUT_DIR, 'tactile')\n",
    "image_cfg, image_encoder, image_transform = init_encoder_info(device, IMAGE_ENCODER_OUT_DIR, 'image')\n",
    "\n",
    "np_means = np.asarray(VISION_IMAGE_MEANS)\n",
    "np_stds = np.asarray(VISION_IMAGE_STDS)\n",
    "\n",
    "inv_image_transform = T.Compose([\n",
    "    T.Normalize(mean = [0,0,0], std = 1 / np_stds ), \n",
    "    T.Normalize(mean = -np_means, std = [1,1,1])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tactile_representation(tactile_values, tactile_encoder): # tactile_values.shape: (15,16,3)\n",
    "    def _get_single_tactile_image(tactile_value):\n",
    "        tactile_image = torch.FloatTensor(tactile_value) # tactile_value.shape: (16,3)\n",
    "        tactile_image = tactile_image.view(4,4,3)\n",
    "        tactile_image = torch.permute(tactile_image, (2,0,1))\n",
    "        return tactile_transform(tactile_image)\n",
    "\n",
    "    for sensor_id in range(len(tactile_values)):\n",
    "        curr_tactile_value = tactile_values[sensor_id]\n",
    "        curr_tactile_image = _get_single_tactile_image(curr_tactile_value).unsqueeze(0) # To make it as if it's a batch\n",
    "        if sensor_id == 0:\n",
    "            curr_repr = tactile_encoder(curr_tactile_image).squeeze() # shape: (64)\n",
    "        else:\n",
    "            curr_repr =  torch.cat([curr_repr, tactile_encoder(curr_tactile_image).squeeze()], dim=0)\n",
    "        print('curr_repr.shape: {}'.format(curr_repr.shape))\n",
    "\n",
    "    return curr_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr_data = load_data(REPR_ROOTS)\n",
    "test_data = load_data(TEST_ROOTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_repr.shape: torch.Size([64])\n",
      "curr_repr.shape: torch.Size([128])\n",
      "curr_repr.shape: torch.Size([192])\n",
      "curr_repr.shape: torch.Size([256])\n",
      "curr_repr.shape: torch.Size([320])\n",
      "curr_repr.shape: torch.Size([384])\n",
      "curr_repr.shape: torch.Size([448])\n",
      "curr_repr.shape: torch.Size([512])\n",
      "curr_repr.shape: torch.Size([576])\n",
      "curr_repr.shape: torch.Size([640])\n",
      "curr_repr.shape: torch.Size([704])\n",
      "curr_repr.shape: torch.Size([768])\n",
      "curr_repr.shape: torch.Size([832])\n",
      "curr_repr.shape: torch.Size([896])\n",
      "curr_repr.shape: torch.Size([960])\n"
     ]
    }
   ],
   "source": [
    "DEMO_ID = 0\n",
    "TACTILE_ID = 2000 #2126\n",
    "# SENSOR_ID = 7\n",
    "tactile_values = test_data['tactile']['values'][DEMO_ID][TACTILE_ID]\n",
    "tactile_repr = get_tactile_representation(tactile_values, tactile_single_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([960])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tactile_repr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tactile_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36852de55b288c46ba617fd48cf310240e4201e2f57004cbdac030fa23152bd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
