{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to visualize demonstrations according to the timestamps\n",
    "# First get the first timestamp of the xela sensor\n",
    "# Then with given fps given get image frames - find closest tactile and image observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tactile_learning.datasets.preprocess import dump_video_to_images, get_closest_id\n",
    "from tactile_learning.utils.visualization import plot_tactile_sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decimal\n",
    "\n",
    "# Get the metadata of images and tactile information\n",
    "def get_desired_indices(root, fps): # frames per second from the video to receive\n",
    "    image_metadata_path = os.path.join(root, 'cam_0_rgb_video.metadata')\n",
    "    tactile_info_path = os.path.join(root, 'touch_sensor_values.h5')\n",
    "\n",
    "    with open(image_metadata_path, 'rb') as f:\n",
    "        image_metadata = pickle.load(f)\n",
    "        image_timestamps_array = np.asarray(image_metadata['timestamps'])\n",
    "        image_timestamps = np.asarray(image_metadata['timestamps']) / 1000.\n",
    "    with h5py.File(tactile_info_path, 'r') as f:\n",
    "        tactile_timestamps = f['timestamps'][()]\n",
    "\n",
    "    image_id, tactile_id = 0, 0\n",
    "    curr_timestamp = tactile_timestamps[0] # These timestamps are in seconds\n",
    "    image_id = get_closest_id(image_id, curr_timestamp, image_timestamps)\n",
    "\n",
    "    tactile_indices, image_indices = [], []\n",
    "    tactile_indices.append(tactile_id)\n",
    "    image_indices.append(image_id)\n",
    "\n",
    "    frame_period = 1. / fps\n",
    "    while(True):\n",
    "        curr_timestamp += frame_period\n",
    "        tactile_id = get_closest_id(tactile_id, curr_timestamp, tactile_timestamps)\n",
    "        image_id = get_closest_id(image_id, curr_timestamp, image_timestamps)\n",
    "\n",
    "        if curr_timestamp > tactile_timestamps[tactile_id] and curr_timestamp > image_timestamps[image_id]:\n",
    "            break\n",
    "\n",
    "        tactile_indices.append(tactile_id)\n",
    "        image_indices.append(image_id)\n",
    "\n",
    "    assert len(tactile_indices) == len(image_indices)\n",
    "    return tactile_indices, image_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dump_states(root, tactile_indices, image_indices):\n",
    "    # Make directory to dump the visualization\n",
    "    pbar = tqdm(total=len(tactile_indices))\n",
    "\n",
    "    with h5py.File(os.path.join(root, 'touch_sensor_values.h5'), 'r') as f:\n",
    "        all_tactile_values = f['sensor_values'][()]\n",
    "\n",
    "    viz_dir = os.path.join(root, 'visualization')\n",
    "    os.makedirs(viz_dir, exist_ok=True)\n",
    "    \n",
    "    video_path = os.path.join(root, f'cam_0_rgb_video.avi')\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    success, image = vidcap.read()\n",
    "    frame_count = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_id = 0\n",
    "    for i in range(len(tactile_indices)):\n",
    "        tactile_id, image_id = tactile_indices[i], image_indices[i]\n",
    "        while frame_id != image_id and success:\n",
    "            # Find the frame that is equal to image_id\n",
    "            success, image = vidcap.read()\n",
    "            frame_id += 1\n",
    "        dump_demo_state(\n",
    "            frame_id = i,\n",
    "            viz_dir = viz_dir,\n",
    "            tactile_values = all_tactile_values[tactile_id,:,:,:],\n",
    "            camera_img = image\n",
    "        )\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "def dump_demo_state(frame_id, viz_dir, tactile_values, camera_img):\n",
    "    # tactile_values: (15,16,3)\n",
    "    fig, axs = plt.subplots(figsize=(20,20), nrows=4, ncols=4)\n",
    "    for row_id in range(4):\n",
    "        for column_id in range(4):\n",
    "            if row_id + column_id > 0: # The top left axis should stay empty\n",
    "                plot_tactile_sensor(\n",
    "                    ax = axs[column_id][row_id],\n",
    "                    sensor_values = tactile_values[row_id*4 + column_id-1],\n",
    "                    title = f'Sensor {row_id*4+column_id-1}'\n",
    "                )\n",
    "    fig.suptitle('Tactile State')\n",
    "    fig.savefig(os.path.join(viz_dir, 'Tactile State.png'))\n",
    "    fig.clf()\n",
    "    plt.close()\n",
    "\n",
    "    tactile_img = cv2.imread(os.path.join(viz_dir, 'Tactile State.png'))\n",
    "    height_scale = camera_img.shape[0] / tactile_img.shape[0]\n",
    "    tactile_img = cv2.resize(\n",
    "        tactile_img,\n",
    "        (int(tactile_img.shape[1] * height_scale),\n",
    "         int(tactile_img.shape[0] * height_scale))\n",
    "    )\n",
    "    total_img = cv2.hconcat([camera_img, tactile_img])\n",
    "\n",
    "    img_name = 'state_{}.png'.format(str(frame_id).zfill(3))\n",
    "    cv2.imwrite(os.path.join(viz_dir, img_name), total_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tactile_indices: [0, 51, 98, 149, 196, 246, 294, 345, 394, 444, 492, 542, 592, 642, 691, 739, 788, 837, 887, 935, 984, 1035, 1089, 1132, 1183, 1232], image_indices: [70, 85, 100, 115, 130, 145, 160, 175, 190, 205, 220, 235, 250, 265, 280, 295, 310, 325, 340, 355, 370, 384, 398, 413, 428, 443]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:27<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# root = '/home/irmak/Workspace/Holo-Bot/extracted_data/both_robots/demonstration_new_box_opening_1'\n",
    "# roots = [\n",
    "#     '/home/irmak/Workspace/Holo-Bot/extracted_data/both_robots/demonstration_new_box_opening_1',\n",
    "#     '/home/irmak/Workspace/Holo-Bot/extracted_data/both_robots/demonstration_hammer_1',\n",
    "#     '/home/irmak/Workspace/Holo-Bot/extracted_data/both_robots/demonstration_new_book_1',\n",
    "#     '/home/irmak/Workspace/Holo-Bot/extracted_data/both_robots/demonstration_bottle_opening_1',\n",
    "#     '/home/irmak/Workspace/Holo-Bot/extracted_data/both_robots/demonstration_new_reverse_peg_4',\n",
    "#     '/home/irmak/Workspace/Holo-Bot/extracted_data/both_robots/demonstration_new_joystick_2'\n",
    "# ]\n",
    "roots = [\n",
    "    # '/home/irmak/Workspace/Holo-Bot/extracted_data/both_robots/demonstration_handle_grabbing_1',\n",
    "    # '/home/irmak/Workspace/Holo-Bot/extracted_data/both_robots/demonstration_bowl_handle_lifting_4',\n",
    "    # '/home/irmak/Workspace/Holo-Bot/extracted_data/both_robots/demonstration_bowl_handle_lifting_6',\n",
    "    # '/home/irmak/Workspace/Holo-Bot/deployment_data/box_handle_lifting/demonstration_4cm_forward_start',\n",
    "    # '/home/irmak/Workspace/Holo-Bot/deployment_data/box_handle_lifting/demonstration_10cm_forward_start',\n",
    "    # '/home/irmak/Workspace/Holo-Bot/deployment_data/box_handle_lifting/demonstration_10cm_left_start',\n",
    "    # '/home/irmak/Workspace/Holo-Bot/deployment_data/box_handle_lifting/demonstration_10cm_right_start',\n",
    "    # '/home/irmak/Workspace/Holo-Bot/deployment_data/box_handle_lifting/demonstration_5cm_up_start',\n",
    "    # '/home/irmak/Workspace/Holo-Bot/deployment_data/box_handle_lifting/demonstration_image_only_repr_home_box_forward'\n",
    "    '/home/irmak/Workspace/Holo-Bot/extracted_data/box_handle_lifting/box_location_changing/eval/demonstration_5'\n",
    "]\n",
    "\n",
    "retrieval_fps = 2\n",
    "for root in roots:\n",
    "    # print('root: {}'.format(root))\n",
    "    tactile_indices, image_indices = get_desired_indices(root=root, fps=retrieval_fps)\n",
    "    print('tactile_indices: {}, image_indices: {}'.format(tactile_indices, image_indices))\n",
    "    dump_states(root, tactile_indices, image_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumping video in root: /home/irmak/Workspace/Holo-Bot/extracted_data/box_handle_lifting/box_location_changing/eval/demonstration_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 5.1.2 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 10.4.0 (conda-forge gcc 10.4.0-18)\n",
      "  configuration: --prefix=/home/irmak/miniconda3/envs/tactile_learning --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1666357487580/_build_env/bin/x86_64-conda-linux-gnu-cc --cxx=/home/conda/feedstock_root/build_artifacts/ffmpeg_1666357487580/_build_env/bin/x86_64-conda-linux-gnu-c++ --nm=/home/conda/feedstock_root/build_artifacts/ffmpeg_1666357487580/_build_env/bin/x86_64-conda-linux-gnu-nm --ar=/home/conda/feedstock_root/build_artifacts/ffmpeg_1666357487580/_build_env/bin/x86_64-conda-linux-gnu-ar --disable-doc --disable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libfontconfig --enable-libopenh264 --enable-gnutls --enable-libmp3lame --enable-libvpx --enable-pthreads --enable-vaapi --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1666357487580/_build_env/bin/pkg-config\n",
      "  libavutil      57. 28.100 / 57. 28.100\n",
      "  libavcodec     59. 37.100 / 59. 37.100\n",
      "  libavformat    59. 27.100 / 59. 27.100\n",
      "  libavdevice    59.  7.100 / 59.  7.100\n",
      "  libavfilter     8. 44.100 /  8. 44.100\n",
      "  libswscale      6.  7.100 /  6.  7.100\n",
      "  libswresample   4.  7.100 /  4.  7.100\n",
      "  libpostproc    56.  6.100 / 56.  6.100\n",
      "[image2 @ 0x561f530c3640] Pattern type 'glob_sequence' is deprecated: use pattern_type 'glob' instead\n",
      "Input #0, image2, from '/home/irmak/Workspace/Holo-Bot/extracted_data/box_handle_lifting/box_location_changing/eval/demonstration_5/visualization/%*.png':\n",
      "  Duration: 00:00:01.08, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgba(pc), 2000x2000 [SAR 3937:3937 DAR 1:1], 25 fps, 25 tbr, 25 tbn\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x561f530ca8c0] using SAR=1/1\n",
      "[libx264 @ 0x561f530ca8c0] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x561f530ca8c0] profile High 4:4:4 Predictive, level 4.0, 4:4:4, 8-bit\n",
      "[libx264 @ 0x561f530ca8c0] 264 - core 164 r3095 baee400 - H.264/MPEG-4 AVC codec - Copyleft 2003-2022 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=22 lookahead_threads=3 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=10 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to '/home/irmak/Workspace/Holo-Bot/extracted_data/box_handle_lifting/box_location_changing/eval/demonstration_5/visualization.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf59.27.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv444p(tv, progressive), 2000x720 [SAR 1:1 DAR 25:9], q=2-31, 10 fps, 10240 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc59.37.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame=   27 fps=0.0 q=-1.0 Lsize=     761kB time=00:00:02.40 bitrate=2597.2kbits/s speed=3.31x    \n",
      "video:760kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.154371%\n",
      "[libx264 @ 0x561f530ca8c0] frame I:1     Avg QP:22.39  size: 50004\n",
      "[libx264 @ 0x561f530ca8c0] frame P:7     Avg QP:19.94  size: 49801\n",
      "[libx264 @ 0x561f530ca8c0] frame B:19    Avg QP:22.03  size: 19931\n",
      "[libx264 @ 0x561f530ca8c0] consecutive B-frames:  3.7%  7.4%  0.0% 88.9%\n",
      "[libx264 @ 0x561f530ca8c0] mb I  I16..4: 27.8% 43.7% 28.5%\n",
      "[libx264 @ 0x561f530ca8c0] mb P  I16..4:  3.5% 24.7%  2.8%  P16..4: 20.7% 11.4%  6.8%  0.0%  0.0%    skip:30.1%\n",
      "[libx264 @ 0x561f530ca8c0] mb B  I16..4:  0.8%  8.7%  0.2%  B16..8: 23.0%  6.1%  1.7%  direct: 4.8%  skip:54.8%  L0:44.5% L1:46.5% BI: 9.0%\n",
      "[libx264 @ 0x561f530ca8c0] 8x8 transform intra:76.1% inter:82.5%\n",
      "[libx264 @ 0x561f530ca8c0] coded y,u,v intra: 59.5% 27.6% 25.6% inter: 20.4% 9.8% 9.3%\n",
      "[libx264 @ 0x561f530ca8c0] i16 v,h,dc,p: 42% 32% 13% 12%\n",
      "[libx264 @ 0x561f530ca8c0] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 17% 19% 47%  3%  3%  3%  2%  2%  4%\n",
      "[libx264 @ 0x561f530ca8c0] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 38% 32% 12%  2%  3%  3%  4%  2%  4%\n",
      "[libx264 @ 0x561f530ca8c0] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x561f530ca8c0] ref P L0: 38.0% 10.7% 32.1% 19.2%\n",
      "[libx264 @ 0x561f530ca8c0] ref B L0: 71.4% 23.6%  5.0%\n",
      "[libx264 @ 0x561f530ca8c0] ref B L1: 87.3% 12.7%\n",
      "[libx264 @ 0x561f530ca8c0] kb/s:2303.13\n"
     ]
    }
   ],
   "source": [
    "# Turn the images to a video and delete the directory\n",
    "video_fps = 10\n",
    "for root in roots:\n",
    "    print('dumping video in root: {}'.format(root))\n",
    "    video_path = os.path.join(root, 'visualization.mp4')\n",
    "    if os.path.exists(video_path):\n",
    "        os.remove(video_path)\n",
    "    viz_dir = os.path.join(root, 'visualization')\n",
    "    os.system('ffmpeg -r {} -i {}/%*.png -vf scale=2000x720,setsar=1:1 {}'.format(\n",
    "        video_fps, # fps\n",
    "        viz_dir,\n",
    "        video_path\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the img directory\n",
    "\n",
    "for root in roots:\n",
    "    viz_dir = os.path.join(root, 'visualization')\n",
    "    shutil.rmtree(viz_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tactile_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36852de55b288c46ba617fd48cf310240e4201e2f57004cbdac030fa23152bd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
