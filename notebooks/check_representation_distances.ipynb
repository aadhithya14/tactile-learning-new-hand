{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Notebook to check representation distances\n",
    "import glob\n",
    "import os\n",
    "import hydra\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from agent.encoder import Encoder\n",
    "\n",
    "from tactile_learning.models import *\n",
    "from tactile_learning.utils import *\n",
    "from tactile_learning.tactile_data import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random episode\n",
    "fn = '/home/irmak/Workspace/tactile-learning/buffer/20230507T204720_14_76.npz'\n",
    "with open(fn, 'rb') as f:\n",
    "    episode = np.load(f)\n",
    "    episode = {k: episode[k] for k in episode.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixels.shape: (77, 3, 224, 224)\n",
      "tactile.shape: (77, 1024)\n"
     ]
    }
   ],
   "source": [
    "print('pixels.shape: {}'.format(episode['pixels'].shape))\n",
    "print('tactile.shape: {}'.format(episode['tactile'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/irmak/Workspace/Holo-Bot/extracted_data/bowl_picking/after_rss' \n",
    "expert_demo_num = 24\n",
    "roots = sorted(glob.glob(f'{data_path}/demonstration_*'))\n",
    "data = load_data(roots, demos_to_use=[expert_demo_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to analyze data\n",
    "class RepresentationAnalyzer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        data_path = '/home/irmak/Workspace/Holo-Bot/extracted_data/bowl_picking/after_rss',\n",
    "        tactile_out_dir = '/home/irmak/Workspace/tactile-learning/tactile_learning/out/2023.01.28/12-32_tactile_byol_bs_512_tactile_play_data_alexnet_pretrained_duration_120',\n",
    "        image_out_dir = '/home/irmak/Workspace/tactile-learning/tactile_learning/out/2023.05.06/10-50_image_byol_bs_32_epochs_500_lr_1e-05_bowl_picking_after_rss',\n",
    "        device = 'cuda',\n",
    "    ):\n",
    "        # Set expert demo\n",
    "        # roots = sorted(glob.glob(f'{data_path}/demonstration_*'))\n",
    "        # self.data = load_data(roots, demos_to_use=[expert_demo_num])\n",
    "        self.data = data\n",
    "        self.data_path = data_path\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        image_cfg, self.image_encoder, self.image_transform = init_encoder_info(self.device, image_out_dir, 'image')\n",
    "        self.inv_image_transform = get_inverse_image_norm() \n",
    "        self.image_normalize = T.Normalize(VISION_IMAGE_MEANS, VISION_IMAGE_STDS)\n",
    "\n",
    "        tactile_cfg, self.tactile_encoder, _ = init_encoder_info(self.device, tactile_out_dir, 'tactile', model_type='byol')\n",
    "        tactile_img = TactileImage(\n",
    "            tactile_image_size = tactile_cfg.tactile_image_size, \n",
    "            shuffle_type = None\n",
    "        )\n",
    "        self.tactile_repr = TactileRepresentation( # This will be used when calculating the reward - not getting the observations\n",
    "            encoder_out_dim = tactile_cfg.encoder.out_dim,\n",
    "            tactile_encoder = self.tactile_encoder,\n",
    "            tactile_image = tactile_img,\n",
    "            representation_type = 'tdex'\n",
    "        )\n",
    "\n",
    "        self.rewards = 'sinkhorn_cosine'\n",
    "        self.sinkhorn_rew_scale = 200\n",
    "\n",
    "        self.reward_representations = ['image', 'tactile']\n",
    "        self.policy_representations = ['image', 'tactile', 'features']\n",
    "\n",
    "        self._set_expert_demo()\n",
    "\n",
    "    def _load_dataset_image(self, demo_id, image_id):\n",
    "        dset_img = load_dataset_image(self.data_path, demo_id, image_id, self.view_num)\n",
    "        img = self.image_transform(dset_img)\n",
    "        return torch.FloatTensor(img) \n",
    "\n",
    "    def _set_expert_demo(self):\n",
    "        # We'll stack the tactile repr and the image observations\n",
    "        self.expert_demo = dict(\n",
    "            image_obs = [], \n",
    "            tactile_repr = []\n",
    "        )\n",
    "        for step_id in range(len(self.data['image']['indices'])): \n",
    "            demo_id, tactile_id = self.data['tactile']['indices'][step_id]\n",
    "\n",
    "            tactile_value = self.data['tactile']['values'][demo_id][tactile_id]\n",
    "            tactile_repr = self.tactile_repr.get(tactile_value, detach=False)\n",
    "\n",
    "            _, image_id = self.data['image']['indices'][step_id]\n",
    "            image = load_dataset_image(\n",
    "                data_path = self.data_path, \n",
    "                demo_id = demo_id, \n",
    "                image_id = image_id,\n",
    "                view_num = 1,\n",
    "                transform = self.image_transform\n",
    "            )\n",
    "\n",
    "            # if step_id == 0:\n",
    "            #     tactile_reprs = tactile_repr.unsqueeze(0)\n",
    "            #     image_obs = image.unsqueeze(0)\n",
    "            # else:\n",
    "            #     image_obs = torch.concat([image_obs, image.unsqueeze(0)], dim=0)\n",
    "            #\n",
    "            #      tactile_reprs = torch.concat([tactile_reprs, tactile_repr.unsqueeze(0)], dim=0)\n",
    "            self.expert_demo['image_obs'].append(image)\n",
    "            self.expert_demo['tactile_repr'].append(tactile_repr)\n",
    "\n",
    "\n",
    "        # self.expert_demo = dict(\n",
    "        #     image_obs = image_obs, \n",
    "        #     tactile_repr = tactile_reprs\n",
    "        # )\n",
    "        for obs_type in self.expert_demo.keys():\n",
    "            self.expert_demo[obs_type] = torch.stack(self.expert_demo[obs_type], 0)\n",
    "\n",
    "    def _get_representation_distances(self, episode_obs, mock=False):\n",
    "        curr_reprs, exp_reprs = [], []\n",
    "        if 'image' in self.reward_representations: # We will not be using features for reward for sure\n",
    "            if mock:\n",
    "                image_reprs = self.image_encoder(episode_obs['image_obs'].to(self.device))\n",
    "            else: \n",
    "                image_obs = self.image_normalize(episode_obs['image_obs']).to(self.device) # This will give all the image observations of one episode\n",
    "                image_reprs = self.image_encoder(image_obs)\n",
    "            expert_image_reprs = self.image_encoder(self.expert_demo['image_obs'].to(self.device))\n",
    "            curr_reprs.append(image_reprs)\n",
    "            exp_reprs.append(expert_image_reprs)\n",
    "    \n",
    "            del image_reprs\n",
    "            del expert_image_reprs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if 'tactile' in self.reward_representations:\n",
    "            tactile_reprs = episode_obs['tactile_repr'].to(self.device) # This will give all the representations of one episode\n",
    "            expert_tactile_reprs = self.expert_demo['tactile_repr'].to(self.device)\n",
    "            curr_reprs.append(tactile_reprs)\n",
    "            exp_reprs.append(expert_tactile_reprs)\n",
    "\n",
    "            del tactile_reprs\n",
    "            del expert_tactile_reprs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Concatenate everything now\n",
    "        obs = torch.concat(curr_reprs, dim=-1).detach()\n",
    "        exp = torch.concat(exp_reprs, dim=-1).detach()\n",
    "\n",
    "        # Get the rewards\n",
    "        if self.rewards == 'sinkhorn_cosine':\n",
    "            cost_matrix = cosine_distance(\n",
    "                obs, exp)  # Get cost matrix for samples using critic network.\n",
    "            print('cost_matrix.shape: {}'.format(cost_matrix.shape))\n",
    "            transport_plan = optimal_transport_plan(\n",
    "                obs, exp, cost_matrix, method='sinkhorn',\n",
    "                niter=100).float()  # Getting optimal coupling\n",
    "            print('ot plan: {}'.format(transport_plan.shape))\n",
    "            ot_rewards = -self.sinkhorn_rew_scale * torch.diag(\n",
    "                torch.mm(transport_plan,\n",
    "                            cost_matrix.T)).detach().cpu().numpy()\n",
    "\n",
    "        episode_obs = {episode_obs[k].detach().cpu() for k in episode_obs.keys()}\n",
    "        print('ot_rewards: {}'.format(ot_rewards))\n",
    "\n",
    "        del obs\n",
    "        del exp\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return ot_rewards\n",
    "\n",
    "    def ot_rewarder(self, episode_obs, mock=False): # TODO: Delete the mock option\n",
    "        \n",
    "        # NOTE: In this code we're not using target encoder since the encoders are already frozen\n",
    "        curr_reprs, exp_reprs = [], []\n",
    "        if 'image' in self.reward_representations: # We will not be using features for reward for sure\n",
    "            if mock:\n",
    "                image_reprs = self.image_encoder(episode_obs['image_obs'].to(self.device))\n",
    "            else:\n",
    "                image_obs = self.image_normalize(episode_obs['image_obs']).to(self.device) # This will give all the image observations of one episode\n",
    "                image_reprs = self.image_encoder(image_obs)\n",
    "            expert_image_reprs = self.image_encoder(self.expert_demo['image_obs'].to(self.device))\n",
    "            curr_reprs.append(image_reprs)\n",
    "            exp_reprs.append(expert_image_reprs)\n",
    "\n",
    "            del image_reprs\n",
    "            del expert_image_reprs\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "        if 'tactile' in self.reward_representations:\n",
    "            tactile_reprs = episode_obs['tactile_repr'].to(self.device) # This will give all the representations of one episode\n",
    "            expert_tactile_reprs = self.expert_demo['tactile_repr'].to(self.device)\n",
    "            curr_reprs.append(tactile_reprs)\n",
    "            exp_reprs.append(expert_tactile_reprs)\n",
    "\n",
    "            del tactile_reprs\n",
    "            del expert_tactile_reprs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Concatenate everything now\n",
    "        obs = torch.concat(curr_reprs, dim=-1).detach()\n",
    "        exp = torch.concat(exp_reprs, dim=-1).detach()\n",
    "\n",
    "        if self.rewards == 'sinkhorn_cosine':\n",
    "            cost_matrix = cosine_distance(\n",
    "                obs, exp)  # Get cost matrix for samples using critic network.\n",
    "            transport_plan = optimal_transport_plan(\n",
    "                obs, exp, cost_matrix, method='sinkhorn',\n",
    "                niter=100).float()  # Getting optimal coupling\n",
    "            ot_rewards = -self.sinkhorn_rew_scale * torch.diag(\n",
    "                torch.mm(transport_plan,\n",
    "                            cost_matrix.T)).detach().cpu().numpy()\n",
    "            \n",
    "        elif self.rewards == 'sinkhorn_euclidean':\n",
    "            cost_matrix = euclidean_distance(\n",
    "                obs, exp)  # Get cost matrix for samples using critic network.\n",
    "            transport_plan = optimal_transport_plan(\n",
    "                obs, exp, cost_matrix, method='sinkhorn',\n",
    "                niter=100).float()  # Getting optimal coupling\n",
    "            ot_rewards = -self.sinkhorn_rew_scale * torch.diag(\n",
    "                torch.mm(transport_plan,\n",
    "                            cost_matrix.T)).detach().cpu().numpy()\n",
    "            \n",
    "        elif self.rewards == 'cosine':\n",
    "            exp = torch.cat((exp, exp[-1].unsqueeze(0)))\n",
    "            ot_rewards = -(1. - F.cosine_similarity(obs, exp))\n",
    "            ot_rewards *= self.sinkhorn_rew_scale\n",
    "            ot_rewards = ot_rewards.detach().cpu().numpy()\n",
    "            \n",
    "        elif self.rewards == 'euclidean':\n",
    "            exp = torch.cat((exp, exp[-1].unsqueeze(0)))\n",
    "            ot_rewards = -(obs - exp).norm(dim=1)\n",
    "            ot_rewards *= self.sinkhorn_rew_scale\n",
    "            ot_rewards = ot_rewards.detach().cpu().numpy()\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        del obs\n",
    "        del exp \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return ot_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod_name: collections, name: OrderedDict\n",
      "mod_name: torch._utils, name: _rebuild_parameter\n",
      "mod_name: torch._utils, name: _rebuild_tensor_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/irmak/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod_name: collections, name: OrderedDict\n",
      "mod_name: torch._utils, name: _rebuild_parameter\n",
      "mod_name: torch._utils, name: _rebuild_tensor_v2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "repr_analyzer = RepresentationAnalyzer(data=data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost_matrix.shape: torch.Size([77, 69])\n",
      "ot plan: torch.Size([77, 69])\n",
      "ot_rewards: [-1.5194867 -1.5211235 -1.5284127 -1.5192441 -1.5157703 -1.5184269\n",
      " -1.5145341 -1.5080873 -1.5011551 -1.5019797 -1.4989655 -1.5020616\n",
      " -1.5034844 -1.5049654 -1.5041409 -1.5033431 -1.5006042 -1.4998842\n",
      " -1.4976026 -1.4977747 -1.507434  -1.5014273 -1.5012411 -1.5047222\n",
      " -1.501378  -1.5022033 -1.5066106 -1.5053742 -1.5074621 -1.508056\n",
      " -1.5081058 -1.505208  -1.5077295 -1.5188035 -1.510505  -1.5181931\n",
      " -1.508751  -1.5083652 -1.5090806 -1.5085036 -1.5107883 -1.5097964\n",
      " -1.5140144 -1.5099934 -1.5187448 -1.5117332 -1.5104812 -1.5064644\n",
      " -1.5051708 -1.50601   -1.5093814 -1.5079747 -1.5083027 -1.5099336\n",
      " -1.5108486 -1.5089498 -1.5089706 -1.5071304 -1.5086879 -1.506789\n",
      " -1.5062562 -1.5037047 -1.4930328 -1.490211  -1.4953387 -1.494063\n",
      " -1.4947919 -1.498822  -1.4954224 -1.4972954 -1.496622  -1.4972457\n",
      " -1.4970027 -1.4968832 -1.4960258 -1.496317  -1.4962063]\n"
     ]
    }
   ],
   "source": [
    "episode_obs = dict(\n",
    "    image_obs = torch.FloatTensor(episode['pixels']),\n",
    "    tactile_repr = torch.FloatTensor(episode['tactile'])\n",
    ")\n",
    "ot_rewards = repr_analyzer._get_representation_distances(episode_obs, mock=False)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost_matrix.shape: torch.Size([77, 69])\n",
      "ot plan: torch.Size([77, 69])\n",
      "ot_rewards: [-0.13105169 -0.13119285 -0.13182153 -0.13103075 -0.13073117 -0.13096027\n",
      " -0.13062453 -0.13006851 -0.12947063 -0.12954175 -0.12928179 -0.12954882\n",
      " -0.12967153 -0.12979926 -0.12972815 -0.12965934 -0.12942313 -0.12936103\n",
      " -0.12916423 -0.12917908 -0.13001218 -0.12949412 -0.12947805 -0.1297783\n",
      " -0.12948987 -0.12956105 -0.12994115 -0.12983452 -0.1300146  -0.13006581\n",
      " -0.1300701  -0.12982018 -0.13003765 -0.13099276 -0.13027702 -0.13094011\n",
      " -0.13012576 -0.13009249 -0.13015419 -0.13010442 -0.13030148 -0.13021591\n",
      " -0.13057971 -0.13023292 -0.1309877  -0.13038296 -0.130275   -0.12992854\n",
      " -0.12981698 -0.12988935 -0.13018014 -0.13005881 -0.13008709 -0.13022776\n",
      " -0.13030668 -0.1301429  -0.1301447  -0.12998599 -0.1301203  -0.12995654\n",
      " -0.12991059 -0.12969053 -0.12877011 -0.12852673 -0.12896898 -0.12885897\n",
      " -0.12892182 -0.1292694  -0.12897621 -0.12913774 -0.12907967 -0.12913346\n",
      " -0.1291125  -0.12910219 -0.12902825 -0.12905337 -0.1290438 ]\n",
      "new_rewards_sum: -9.999999046325684\n"
     ]
    }
   ],
   "source": [
    "ot_reward_sum = np.sum(ot_rewards)\n",
    "repr_analyzer.sinkhorn_rew_scale = repr_analyzer.sinkhorn_rew_scale * 10 / float(\n",
    "    np.abs(ot_reward_sum))\n",
    "new_rewards = repr_analyzer._get_representation_distances(\n",
    "    episode_obs = episode_obs\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "new_rewards_sum = np.sum(new_rewards)\n",
    "print(f'new_rewards_sum: {new_rewards_sum}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_obs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rewards from an actual demo\n",
    "mock_data = load_data(roots, demos_to_use=[29])\n",
    "# We'll stack the tactile repr and the image observations\n",
    "mock_episode_obs = dict(\n",
    "    image_obs = [],\n",
    "    tactile_repr = []\n",
    ")\n",
    "for step_id in range(len(mock_data['image']['indices'])): \n",
    "    demo_id, tactile_id = mock_data['tactile']['indices'][step_id]\n",
    "\n",
    "    tactile_value = mock_data['tactile']['values'][demo_id][tactile_id]\n",
    "    tactile_repr = repr_analyzer.tactile_repr.get(tactile_value, detach=False)\n",
    "\n",
    "    _, image_id = mock_data['image']['indices'][step_id]\n",
    "    image = load_dataset_image(\n",
    "        data_path = repr_analyzer.data_path, \n",
    "        demo_id = demo_id, \n",
    "        image_id = image_id,\n",
    "        view_num = 1,\n",
    "        transform = repr_analyzer.image_transform\n",
    "    )\n",
    "\n",
    "    # if step_id == 0:\n",
    "    #     tactile_reprs = tactile_repr.unsqueeze(0)\n",
    "    #     image_obs = image.unsqueeze(0)\n",
    "    # else:\n",
    "    #     image_obs = torch.concat([image_obs, image.unsqueeze(0)], dim=0)\n",
    "    #     tactile_reprs = torch.concat([tactile_reprs, tactile_repr.unsqueeze(0)], dim=0)\n",
    "    mock_episode_obs['image_obs'].append(image)\n",
    "    mock_episode_obs['tactile_repr'].append(tactile_repr)\n",
    "\n",
    "for obs_type in mock_episode_obs.keys():\n",
    "    mock_episode_obs[obs_type] = torch.stack(mock_episode_obs[obs_type], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 3, 480, 480])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_episode_obs['image_obs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 15.74 GiB total capacity; 14.06 GiB already allocated; 9.06 MiB free; 14.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/irmak/Workspace/tactile-learning/notebooks/check_representation_distances.ipynb Cell 12\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_representation_distances.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m repr_analyzer\u001b[39m.\u001b[39;49m_get_representation_distances(episode_obs\u001b[39m=\u001b[39;49mmock_episode_obs, mock\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_representation_distances.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n",
      "\u001b[1;32m/home/irmak/Workspace/tactile-learning/notebooks/check_representation_distances.ipynb Cell 12\u001b[0m in \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_representation_distances.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m     image_obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_normalize(episode_obs[\u001b[39m'\u001b[39m\u001b[39mimage_obs\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39m# This will give all the image observations of one episode\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_representation_distances.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m     image_reprs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_encoder(image_obs)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_representation_distances.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m expert_image_reprs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_encoder(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexpert_demo[\u001b[39m'\u001b[39;49m\u001b[39mimage_obs\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_representation_distances.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m curr_reprs\u001b[39m.\u001b[39mappend(image_reprs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_representation_distances.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m exp_reprs\u001b[39m.\u001b[39mappend(expert_image_reprs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torchvision/models/resnet.py:275\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[1;32m    274\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[0;32m--> 275\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer3(x)\n\u001b[1;32m    276\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(x)\n\u001b[1;32m    278\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torchvision/models/resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     90\u001b[0m     identity \u001b[39m=\u001b[39m x\n\u001b[0;32m---> 92\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m     93\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 15.74 GiB total capacity; 14.06 GiB already allocated; 9.06 MiB free; 14.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "repr_analyzer._get_representation_distances(episode_obs=mock_episode_obs, mock=True)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tactile_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
