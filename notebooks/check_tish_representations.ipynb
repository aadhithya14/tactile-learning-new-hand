{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to check each representation with the given encoders\n",
    "# Will receive:\n",
    "# a list of encoders to try for each task\n",
    "# a list of experts to try the encoders on\n",
    "import os\n",
    "import hydra\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "# from agent.encoder import Encoder\n",
    "\n",
    "from tactile_learning.models import *\n",
    "from tactile_learning.utils import *\n",
    "from tactile_learning.tactile_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:1'\n",
    "TACTILE_OUT_DIR = '/home/irmak/Workspace/tactile-learning/tactile_learning/out/2023.01.28/12-32_tactile_byol_bs_512_tactile_play_data_alexnet_pretrained_duration_120'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tactile_repr_module(device):\n",
    "    tactile_cfg, tactile_encoder, _ = init_encoder_info(\n",
    "        device = device,\n",
    "        out_dir = TACTILE_OUT_DIR,\n",
    "        encoder_type = 'tactile',\n",
    "        model_type='byol'\n",
    "    )\n",
    "    tactile_img = TactileImage(\n",
    "        tactile_image_size = tactile_cfg.tactile_image_size, \n",
    "        shuffle_type = None\n",
    "    )\n",
    "    tactile_repr_module = TactileRepresentation( # This will be used when calculating the reward - not getting the observations\n",
    "        encoder_out_dim = tactile_cfg.encoder.out_dim,\n",
    "        tactile_encoder = tactile_encoder,\n",
    "        tactile_image = tactile_img,\n",
    "        representation_type = 'tdex',\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    return tactile_repr_module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expert_representations_per_encoder(encoder, task_expert_demos, device):\n",
    "    # Traverse through all the experts and get the representations\n",
    "    task_representations = []\n",
    "    for expert_id in range(len(task_expert_demos)):\n",
    "        expert_representations = encoder(task_expert_demos[expert_id]['image_obs'].to(device)) # One trajectory representations\n",
    "        task_representations.append(expert_representations)\n",
    "    \n",
    "    return task_representations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_traj_score(traj1, traj2):\n",
    "    # traj1.shape: (80, 512), traj2.shape: (80,512)\n",
    "    cost_matrix = cosine_distance(\n",
    "            traj1, traj2)  # Get cost matrix for samples using critic network.\n",
    "    transport_plan = optimal_transport_plan(\n",
    "        traj1, traj2, cost_matrix, method='sinkhorn',\n",
    "        niter=100, exponential_weight_init=False).float().detach().cpu().numpy()\n",
    "\n",
    "    max_transport_plan = np.max(transport_plan, axis=1) # We are going to find the maximums for traj1\n",
    "    print('max_transport_plan.shape: {}, traj1.shape: {}, traj2.shape: {}'.format(\n",
    "        max_transport_plan.shape, traj1.shape, traj2.shape\n",
    "    ))\n",
    "    return np.sum(max_transport_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_encoder_score(encoder, all_expert_demos, encoder_id, device): # Will get all the representations and calculate the score of the \n",
    "\n",
    "    all_expert_representations = get_expert_representations_per_encoder(\n",
    "        encoder = encoder,\n",
    "        task_expert_demos = all_expert_demos,\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    # Get combinations of the trajectories and calculate the score for them\n",
    "    score_matrix = np.zeros((5,5))\n",
    "    for i in range(score_matrix.shape[0]):\n",
    "        for j in range(score_matrix.shape[1]):\n",
    "            traj1 = all_expert_representations[i] \n",
    "            traj2 = all_expert_representations[j] \n",
    "            score_matrix[i,j] = calc_traj_score(traj1, traj2)\n",
    "\n",
    "    print('SCORE MATRIX FOR ENCODER: {} \\n{}\\n-----'.format(\n",
    "        encoder_id, \n",
    "        score_matrix\n",
    "    ))\n",
    "\n",
    "    return score_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method to load all the tasks\n",
    "# def load_all_expert_demos(task_names, expert_demo_nums, view_nums, device):\n",
    "#     # Create the tactile repr module - this is the same for all the tasks\n",
    "#     tactile_repr_module = get_tactile_repr_module(device)\n",
    "#     all_experts = []\n",
    "\n",
    "#     for i, task_name in enumerate(task_names):\n",
    "#         root_path = f'/home/irmak/Workspace/Holo-Bot/extracted_data/{task_name}'\n",
    "#         task_expert_demo_nums = expert_demo_nums[i]\n",
    "        \n",
    "#         view_num = view_nums[i]\n",
    "#         def viewed_crop_transform(image):\n",
    "#             return crop_transform(image, camera_view=view_num)\n",
    "#         image_transform =  T.Compose([\n",
    "#             T.Resize((480,640)),\n",
    "#             T.Lambda(viewed_crop_transform),\n",
    "#             T.Resize(480),\n",
    "#             T.ToTensor(),\n",
    "#             T.Normalize(VISION_IMAGE_MEANS, VISION_IMAGE_STDS), \n",
    "#         ])\n",
    "#         task_expert_demos = load_expert_demos_per_task(\n",
    "#             data_path = root_path,\n",
    "#             expert_demo_nums = task_expert_demo_nums,\n",
    "#             tactile_repr_module = tactile_repr_module,\n",
    "#             image_transform = image_transform,\n",
    "#             view_num = view_num\n",
    "#         )\n",
    "\n",
    "#         all_experts.append(\n",
    "#             task_expert_demos\n",
    "#         )\n",
    "\n",
    "#     return all_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# This image transform will have everything\n",
    "def load_expert_demos_per_task(task_name, expert_demo_nums, view_num, device):\n",
    "    data_path = f'/home/irmak/Workspace/Holo-Bot/extracted_data/{task_name}'\n",
    "    roots = sorted(glob.glob(f'{data_path}/demonstration_*'))\n",
    "    data = load_data(roots, demos_to_use=expert_demo_nums) # NOTE: This could be fucked up\n",
    "\n",
    "    # Get the tactile module and the image transform\n",
    "    def viewed_crop_transform(image):\n",
    "        return crop_transform(image, camera_view=view_num)\n",
    "    image_transform =  T.Compose([\n",
    "        T.Resize((480,640)),\n",
    "        T.Lambda(viewed_crop_transform),\n",
    "        T.Resize(480),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(VISION_IMAGE_MEANS, VISION_IMAGE_STDS), \n",
    "    ])\n",
    "    \n",
    "    expert_demos = []\n",
    "    image_obs = [] \n",
    "    old_demo_id = -1\n",
    "    for step_id in range(len(data['image']['indices'])): \n",
    "        demo_id, image_id = data['image']['indices'][step_id]\n",
    "        if (demo_id != old_demo_id and step_id > 0) or (step_id == len(data['image']['indices'])-1): # NOTE: We are losing the last frame of the last expert\n",
    "\n",
    "            expert_demos.append(dict(\n",
    "                image_obs = torch.stack(image_obs, 0), \n",
    "            ))\n",
    "            image_obs = [] \n",
    "\n",
    "        image = load_dataset_image(\n",
    "            data_path = data_path, \n",
    "            demo_id = demo_id, \n",
    "            image_id = image_id,\n",
    "            view_num = view_num,\n",
    "            transform = image_transform\n",
    "        )\n",
    "        image_obs.append(image)\n",
    "        # tactile_reprs.append(tactile_repr)\n",
    "\n",
    "\n",
    "        old_demo_id = demo_id\n",
    "\n",
    "    return expert_demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/irmak/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod_name: collections, name: OrderedDict\n",
      "mod_name: torch._utils, name: _rebuild_parameter\n",
      "mod_name: torch._utils, name: _rebuild_tensor_v2\n"
     ]
    }
   ],
   "source": [
    "# # Load all the expert demos\n",
    "# ALL_EXPERT_DEMOS = load_all_expert_demos(\n",
    "#     task_names = [\n",
    "#         # 'plier_picking',\n",
    "#         'bowl_picking',\n",
    "#         # 'card_flipping',\n",
    "#         # 'card_turning',\n",
    "#         # 'peg_insertion'\n",
    "#     ],\n",
    "#     expert_demo_nums=[\n",
    "#         # [3,10,15,16,20,25],\n",
    "#         [],\n",
    "#         # [24,26,27,31,32,33],\n",
    "#         # [],\n",
    "#         # [] \n",
    "#     ],\n",
    "#     view_nums = [\n",
    "#         # 0, 1, 0, 0, 0\n",
    "#         1\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowl_unstacking_info = dict(\n",
    "    encoders = [\n",
    "        dict(\n",
    "            model_path = '/home/irmak/Workspace/tactile-learning/tactile_learning/out/2023.05.11/13-21_bc_bs_32_epochs_500_lr_1e-05_bowl_picking_after_rss',\n",
    "            model_type = 'bc',\n",
    "            view_num = 1,\n",
    "            encoder_fn = None,\n",
    "            device = 0\n",
    "        ),\n",
    "        dict(\n",
    "            model_path = '/home/irmak/Workspace/tactile-learning/tactile_learning/out/2023.06.06/18-27_temporal_ssl_bs_32_epochs_1000_view_1_bowl_picking_frame_diff_5_resnet',\n",
    "            model_type = 'temporal',\n",
    "            view_num = 1,\n",
    "            encoder_fn = None,\n",
    "            device = 1,\n",
    "        ),\n",
    "        dict(\n",
    "            model_path = '/home/irmak/Workspace/tactile-learning/tactile_learning/out/2023.05.06/10-50_image_byol_bs_32_epochs_500_lr_1e-05_bowl_picking_after_rss',\n",
    "            model_type = 'byol',\n",
    "            view_num = 1,\n",
    "            encoder_fn = None,\n",
    "            device = 2, \n",
    "        ),\n",
    "        dict(\n",
    "            model_path = None,\n",
    "            model_type = 'pretrained',\n",
    "            encoder_fn = resnet18,\n",
    "            view_num = 1,\n",
    "            device = 3\n",
    "        )\n",
    "    ],\n",
    "    demos = dict(\n",
    "        task_name = 'bowl_picking',\n",
    "        expert_demo_nums = [],\n",
    "        view_num = 1  \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_encoder(view_num, model_type, model_path, encoder_fn, device):\n",
    "    # print(kwargs)\n",
    "    # view_num = kwargs['view_num']\n",
    "    # model_type = kwargs['model_type']\n",
    "    # model_path=kwargs['model_path']\n",
    "    # encoder_fn=kwargs['encoder_fn']\n",
    "    # devive\n",
    "    \n",
    "    # device = torch.device(DEVICE)\n",
    "    if model_type == 'pretrained' and not (encoder_fn is None):\n",
    "        # It means that this is pretrained\n",
    "        image_encoder = encoder_fn(pretrained=True, out_dim=512, remove_last_layer=True).to(device)\n",
    "\n",
    "    else:\n",
    "        _, image_encoder, _ = init_encoder_info(\n",
    "            device = device,\n",
    "            out_dir = model_path,\n",
    "            encoder_type = 'image',\n",
    "            view_num = view_num,\n",
    "            model_type = model_type\n",
    "        )\n",
    "\n",
    "    return image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod_name: collections, name: OrderedDict\n",
      "mod_name: torch._utils, name: _rebuild_parameter\n",
      "mod_name: torch._utils, name: _rebuild_tensor_v2\n",
      "mod_name: collections, name: OrderedDict\n",
      "mod_name: torch._utils, name: _rebuild_parameter\n",
      "mod_name: torch._utils, name: _rebuild_tensor_v2\n",
      "mod_name: collections, name: OrderedDict\n",
      "mod_name: torch._utils, name: _rebuild_parameter\n",
      "mod_name: torch._utils, name: _rebuild_tensor_v2\n"
     ]
    }
   ],
   "source": [
    "bowl_unstacking_encoders = [\n",
    "    load_encoder(**encoder_args) for encoder_args in bowl_unstacking_info['encoders']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/irmak/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod_name: collections, name: OrderedDict\n",
      "mod_name: torch._utils, name: _rebuild_parameter\n",
      "mod_name: torch._utils, name: _rebuild_tensor_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/irmak/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod_name: collections, name: OrderedDict\n",
      "mod_name: torch._utils, name: _rebuild_parameter\n",
      "mod_name: torch._utils, name: _rebuild_tensor_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/irmak/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod_name: collections, name: OrderedDict\n",
      "mod_name: torch._utils, name: _rebuild_parameter\n",
      "mod_name: torch._utils, name: _rebuild_tensor_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/irmak/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod_name: collections, name: OrderedDict\n",
      "mod_name: torch._utils, name: _rebuild_parameter\n",
      "mod_name: torch._utils, name: _rebuild_tensor_v2\n"
     ]
    }
   ],
   "source": [
    "bowl_unstacking_demos = [load_expert_demos_per_task(**bowl_unstacking_info['demos'], device=i) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irmak/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/ot/bregman.py:535: UserWarning: Sinkhorn did not converge. You might want to increase the number of iterations `numItermax` or the regularization parameter `reg`.\n",
      "  warnings.warn(\"Sinkhorn did not converge. You might want to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_transport_plan.shape: (73,), traj1.shape: torch.Size([73, 512]), traj2.shape: torch.Size([73, 512])\n",
      "max_transport_plan.shape: (73,), traj1.shape: torch.Size([73, 512]), traj2.shape: torch.Size([69, 512])\n",
      "max_transport_plan.shape: (73,), traj1.shape: torch.Size([73, 512]), traj2.shape: torch.Size([72, 512])\n",
      "max_transport_plan.shape: (73,), traj1.shape: torch.Size([73, 512]), traj2.shape: torch.Size([85, 512])\n",
      "max_transport_plan.shape: (73,), traj1.shape: torch.Size([73, 512]), traj2.shape: torch.Size([80, 512])\n",
      "max_transport_plan.shape: (69,), traj1.shape: torch.Size([69, 512]), traj2.shape: torch.Size([73, 512])\n",
      "max_transport_plan.shape: (69,), traj1.shape: torch.Size([69, 512]), traj2.shape: torch.Size([69, 512])\n",
      "max_transport_plan.shape: (69,), traj1.shape: torch.Size([69, 512]), traj2.shape: torch.Size([72, 512])\n",
      "max_transport_plan.shape: (69,), traj1.shape: torch.Size([69, 512]), traj2.shape: torch.Size([85, 512])\n",
      "max_transport_plan.shape: (69,), traj1.shape: torch.Size([69, 512]), traj2.shape: torch.Size([80, 512])\n",
      "max_transport_plan.shape: (72,), traj1.shape: torch.Size([72, 512]), traj2.shape: torch.Size([73, 512])\n",
      "max_transport_plan.shape: (72,), traj1.shape: torch.Size([72, 512]), traj2.shape: torch.Size([69, 512])\n",
      "max_transport_plan.shape: (72,), traj1.shape: torch.Size([72, 512]), traj2.shape: torch.Size([72, 512])\n",
      "max_transport_plan.shape: (72,), traj1.shape: torch.Size([72, 512]), traj2.shape: torch.Size([85, 512])\n",
      "max_transport_plan.shape: (72,), traj1.shape: torch.Size([72, 512]), traj2.shape: torch.Size([80, 512])\n",
      "max_transport_plan.shape: (85,), traj1.shape: torch.Size([85, 512]), traj2.shape: torch.Size([73, 512])\n",
      "max_transport_plan.shape: (85,), traj1.shape: torch.Size([85, 512]), traj2.shape: torch.Size([69, 512])\n",
      "max_transport_plan.shape: (85,), traj1.shape: torch.Size([85, 512]), traj2.shape: torch.Size([72, 512])\n",
      "max_transport_plan.shape: (85,), traj1.shape: torch.Size([85, 512]), traj2.shape: torch.Size([85, 512])\n",
      "max_transport_plan.shape: (85,), traj1.shape: torch.Size([85, 512]), traj2.shape: torch.Size([80, 512])\n",
      "max_transport_plan.shape: (80,), traj1.shape: torch.Size([80, 512]), traj2.shape: torch.Size([73, 512])\n",
      "max_transport_plan.shape: (80,), traj1.shape: torch.Size([80, 512]), traj2.shape: torch.Size([69, 512])\n",
      "max_transport_plan.shape: (80,), traj1.shape: torch.Size([80, 512]), traj2.shape: torch.Size([72, 512])\n",
      "max_transport_plan.shape: (80,), traj1.shape: torch.Size([80, 512]), traj2.shape: torch.Size([85, 512])\n",
      "max_transport_plan.shape: (80,), traj1.shape: torch.Size([80, 512]), traj2.shape: torch.Size([80, 512])\n",
      "SCORE MATRIX FOR ENCODER: 0 \n",
      "[[0.09589503 0.08996791 0.05719613 0.07603814 0.06259806]\n",
      " [0.08662175 0.13327888 0.06893462 0.09144784 0.07940976]\n",
      " [0.0559314  0.0711052  0.09810291 0.06561193 0.07502556]\n",
      " [0.07920586 0.09908856 0.07596195 0.11300652 0.08170861]\n",
      " [0.0613266  0.08038512 0.07849346 0.07680465 0.10652579]]\n",
      "-----\n",
      "id: 0 encoder_score: [[0.09589503 0.08996791 0.05719613 0.07603814 0.06259806]\n",
      " [0.08662175 0.13327888 0.06893462 0.09144784 0.07940976]\n",
      " [0.0559314  0.0711052  0.09810291 0.06561193 0.07502556]\n",
      " [0.07920586 0.09908856 0.07596195 0.11300652 0.08170861]\n",
      " [0.0613266  0.08038512 0.07849346 0.07680465 0.10652579]]\n",
      "----\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 122.00 MiB (GPU 1; 15.74 GiB total capacity; 14.03 GiB already allocated; 67.06 MiB free; 14.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb Cell 17\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m encoder_id, encoder \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(bowl_unstacking_encoders):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     encoder_score \u001b[39m=\u001b[39m calc_encoder_score(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         encoder \u001b[39m=\u001b[39;49m encoder,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         all_expert_demos \u001b[39m=\u001b[39;49m bowl_unstacking_demos[encoder_id], \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         encoder_id \u001b[39m=\u001b[39;49m encoder_id,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         device \u001b[39m=\u001b[39;49m encoder_id\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mid: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m encoder_score: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(encoder_id, encoder_score))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m----\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb Cell 17\u001b[0m in \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalc_encoder_score\u001b[39m(encoder, all_expert_demos, encoder_id, device): \u001b[39m# Will get all the representations and calculate the score of the \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     all_expert_representations \u001b[39m=\u001b[39m get_expert_representations_per_encoder(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         encoder \u001b[39m=\u001b[39;49m encoder,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         task_expert_demos \u001b[39m=\u001b[39;49m all_expert_demos,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         device \u001b[39m=\u001b[39;49m device\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# Get combinations of the trajectories and calculate the score for them\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     score_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39m5\u001b[39m,\u001b[39m5\u001b[39m))\n",
      "\u001b[1;32m/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb Cell 17\u001b[0m in \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m task_representations \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m expert_id \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(task_expert_demos)):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     expert_representations \u001b[39m=\u001b[39m encoder(task_expert_demos[expert_id][\u001b[39m'\u001b[39;49m\u001b[39mimage_obs\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device)) \u001b[39m# One trajectory representations\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     task_representations\u001b[39m.\u001b[39mappend(expert_representations)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blambda/home/irmak/Workspace/tactile-learning/notebooks/check_tish_representations.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mreturn\u001b[39;00m task_representations\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torchvision/models/resnet.py:274\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[1;32m    273\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[0;32m--> 274\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n\u001b[1;32m    276\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torchvision/models/resnet.py:93\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m identity \u001b[39m=\u001b[39m x\n\u001b[1;32m     92\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[0;32m---> 93\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(out)\n\u001b[1;32m     94\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[1;32m     96\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    169\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    170\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    171\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    172\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    173\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    175\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    176\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    177\u001b[0m     bn_training,\n\u001b[1;32m    178\u001b[0m     exponential_average_factor,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    180\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile_learning/lib/python3.9/site-packages/torch/nn/functional.py:2438\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2435\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2436\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2438\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2439\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2440\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 1; 15.74 GiB total capacity; 14.03 GiB already allocated; 67.06 MiB free; 14.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for encoder_id, encoder in enumerate(bowl_unstacking_encoders):\n",
    "    encoder_score = calc_encoder_score(\n",
    "        encoder = encoder,\n",
    "        all_expert_demos = bowl_unstacking_demos[encoder_id], \n",
    "        encoder_id = encoder_id,\n",
    "        device = encoder_id\n",
    "    )\n",
    "    print('id: {} encoder_score: {}'.format(encoder_id, encoder_score))\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tactile_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
