defaults:
  - suite: gym
  - agent: fish # Optimizer will be initialized inside the agent
  # - dataset: fish_all_replay_buffer - TODO

data_path: /home/irmak/Workspace/Holo-Bot/extracted_data/cup_picking/after_rss
image_out_dir: /home/irmak/Workspace/tactile-learning/tactile_learning/out/2023.04.05/00-59_image_byol_bs_32_cup_picking_after_rss
tactile_out_dir: /home/irmak/Workspace/tactile-learning/tactile_learning/out/2023.01.28/12-32_tactile_byol_bs_512_tactile_play_data_alexnet_pretrained_duration_120
reward_representations: ['tactile']
policy_representations: ['image', 'tactile']

seed: 42 
device: cuda

num_train_frames: 5000 # Total training numbers
num_seed_frames: 20
eval_every_frames: 100

# Environment params
expert_demo_num: 15
mock_env: True # If set to true we will be using mock demos as the environment interaction
mock_demo_nums: [] # Use all of them # [13,15,2,7] # 15 is the expert demo in the agent

# Agent params 
bc_regularize: False

# Replay buffer params
replay_buffer_size: 150000
replay_buffer_num_workers: 2
nstep: 3
batch_size: 256

# Recorder
save_video: False 
save_train_video: False 

log: True
experiment: fish_rewards_mock_demo_training

# hydra configuration - should be received separately
hydra:
    run:
        dir: /home/irmak/Workspace/tactile-learning/tactile_learning/out/${now:%Y.%m.%d}/${now:%H-%M}_${experiment}

